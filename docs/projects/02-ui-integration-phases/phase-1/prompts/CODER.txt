===== PHASE 1: CODER PROMPT =====
ROLE: Senior TypeScript developer implementing phases of the Cody CLI integration project. You write clean, tested code following TDD principles with mocked-service tests at library boundaries.


---

PRODUCT:

**Cody** is a command-line interface for the Codex TypeScript library. It provides multi-provider LLM agent capabilities supporting OpenAI (Responses, Chat) and Anthropic (Messages) APIs with tool execution, conversation persistence, and structured tool calling. Built as a TypeScript port of OpenAI's Rust-based Codex CLI, Cody serves as both a standalone CLI tool and reference implementation for the @openai/codex-core library.


---

PROJECT CONTEXT:

# Project 02: UI Integration & Library Definition

## What We're Building

Project 02 integrates all ported Codex modules (Phases 1-6) into a working command-line interface called **Cody** and defines the library API surface for @openai/codex-core. This project validates the Rust → TypeScript port by wiring protocol, configuration, persistence, execution, client, tools, and orchestration layers into complete conversation flows.

## Why It Matters

The port is functionally complete but untested as an integrated system. Individual modules have unit tests, but we haven't verified end-to-end workflows. This project proves the port works, exposes integration issues, and establishes the library interface that external developers will use.

## Project Success Criteria

By project completion:
- User can create conversations, send messages, receive responses (all providers: OpenAI Responses/Chat, Anthropic Messages)
- All auth methods work (API keys, ChatGPT OAuth, Claude OAuth)
- Tools execute with approval flow
- Conversations persist and resume (JSONL format)
- MCP integration functional
- Library API documented (public exports, usage examples)
- REST API designed (optional implementation)
- Zero-error quality baseline maintained (0 TS errors, 0 ESLint errors, all tests passing)

## Dependencies

- Phase 6 complete (75 modules ported, 1,876 tests passing)
- Phase 5.2 complete (quality baseline clean)
- API keys: OpenAI, Anthropic, OpenRouter
- OAuth tokens: Read from ~/.codex (ChatGPT), ~/.claude (Claude)

## Scope

**In scope:** CLI commands, provider integration (3 APIs), auth methods (4 total), tool execution, persistence/resume, library API docs, REST API spec

**Non-scope:** Script harness (Project 03), memory innovations (Projects 04-06), rich TUI, additional tools, performance optimization, production hardening


---

PHASE 1 TECHNICAL DESIGN:

# Phase 1: Technical Design

**Phase:** Basic Chat Flow
**Goal:** Wire CLI → ConversationManager → Codex → Session → ModelClient for basic conversation

---

## Integration Overview

(From TECH-APPROACH Section 2)

Phase 1 proves core conversation flow works end-to-end. Wire CLI commands to ConversationManager, Manager to Codex, Codex to Session, Session to ModelClient (Responses API only). First time these pieces talk to each other. Single provider (OpenAI Responses), single auth (API key) to minimize variables.

Integration happens in layers: CLI command parser → handler functions → ConversationManager methods. ConversationManager delegates to Codex (wiring exists from port). Codex uses ModelClient (exists from port). Our job: wire CLI → Manager, verify Manager → Codex → Client chain works.

Testing: Mock ModelClient (no real API calls). Verify conversation flow (create → send → receive → history) without network/keys/limits. Fast, deterministic, repeatable.

---

## Actual Signatures (from ported code)

### ConversationManager

Location: `codex-ts/src/core/conversation-manager.ts`

```typescript
class ConversationManager {
  constructor(authManager: AuthManager, sessionSource: unknown)
  
  async newConversation(config: Config): Promise<NewConversation>
  async getConversation(conversationId: ConversationId): Promise<CodexConversation | undefined>
  async resumeConversationFromRollout(rolloutPath: string, config: Config): Promise<NewConversation>
  async removeConversation(conversationId: ConversationId): Promise<void>
}

interface NewConversation {
  conversationId: ConversationId,
  conversation: CodexConversation,
  sessionConfigured: SessionConfiguredEvent
}
```

### CodexConversation

Location: `codex-ts/src/core/codex-conversation.ts`

```typescript
class CodexConversation {
  constructor(codex: Codex, rolloutPath: string)
  
  async submit(input: UserInput[]): Promise<void>
  async nextEvent(): Promise<{msg: EventMsg}>
  rolloutPath(): string
}
```

**For Phase 1 CLI:** We'll wrap CodexConversation.submit() with simpler sendMessage(text: string) helper.

### Config

Location: `codex-ts/src/core/config.ts`

Config is complex (ported from Rust). For Phase 1, we need minimal subset:

```typescript
interface MinimalConfig {
  provider: {
    name: 'openai' | 'anthropic',
    api: 'responses' | 'chat' | 'messages',
    model: string
  },
  auth: {
    method: 'api-key' | 'oauth-chatgpt' | 'oauth-claude',
    openai_key?: string,
    anthropic_key?: string
  }
}
```

Read from: `~/.codex/config.toml`

### AuthManager

Location: `codex-ts/src/core/auth/index.ts`

```typescript
class AuthManager {
  // Already ported, just use it
  async getApiKey(provider: string): Promise<string>
}
```

For Phase 1: Only API key auth. OAuth in Phase 4.

---

## Config File Format

Example `~/.codex/config.toml`:

```toml
[provider]
name = "openai"
api = "responses"
model = "gpt-4"

[auth]
method = "api-key"
openai_key = "sk-proj-..."
```

CLI reads this, constructs Config object, passes to ConversationManager.

If file missing: Use defaults or error. Decision: Log in DECISIONS.md.

---

## Dependency Construction Pattern

**The wiring challenge:** ConversationManager needs AuthManager and sessionSource.

```typescript
// In CLI initialization
import {AuthManager} from '../core/auth';
import {ConversationManager} from '../core/conversation-manager';
import {Config} from '../core/config';

// Load config
const config = await loadConfig('~/.codex/config.toml');

// Create AuthManager
const authManager = new AuthManager(/* ... */);

// Create sessionSource (what is this?)
const sessionSource = null; // TODO: Figure out from port

// Create ConversationManager
const manager = new ConversationManager(authManager, sessionSource);
```

**sessionSource is TODO in port.** For Phase 1: Pass `null` or minimal stub. Document in DECISIONS.md.

---

## CLI Command Implementation

### cody new

```typescript
// src/cli/commands/new.ts
import {program} from 'commander';
import {getManager} from '../manager-singleton';

program
  .command('new')
  .description('Create new conversation')
  .action(async () => {
    const manager = getManager(); // Get or create manager
    const config = await loadConfig();
    
    const {conversationId, conversation} = await manager.newConversation(config);
    
    console.log(`Created conversation: ${conversationId.toString()}`);
    
    // Store active conversation for chat command
    setActiveConversation(conversationId, conversation);
  });
```

### cody chat

```typescript
// src/cli/commands/chat.ts
program
  .command('chat <message>')
  .description('Send message to active conversation')
  .action(async (message: string) => {
    const {conversation} = getActiveConversation();
    
    // Submit message
    const input = [{type: 'text', text: message}]; // UserInput format
    await conversation.submit(input);
    
    // Wait for response
    const event = await conversation.nextEvent();
    
    // Render response
    renderEvent(event.msg);
  });
```

**Note:** CodexConversation uses event-based API (submit → nextEvent loop). CLI can wrap in simpler interface or use as-is. Document choice in DECISIONS.md.

---

## Mock Implementation Guide

### Mock ModelClient

Location: `tests/mocks/model-client.ts`

```typescript
import {vi} from 'vitest';
import type {ResponseItem} from '../../src/protocol/items';

export function createMockClient(responses: ResponseItem[][]) {
  let callIndex = 0;
  
  return {
    async sendMessage(request: any): Promise<ResponseItem[]> {
      if (callIndex >= responses.length) {
        throw new Error('Mock exhausted - more calls than responses provided');
      }
      return responses[callIndex++];
    },
    getModelContextWindow: vi.fn().mockReturnValue(128000),
    getProvider: vi.fn().mockReturnValue({name: 'openai'}),
    // Add other ModelClient methods as needed
  };
}

// Usage
const mockClient = createMockClient([
  // First call response
  [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello!'}]}],
  // Second call response
  [{type: 'message', role: 'assistant', content: [{type: 'text', text: 'How are you?'}]}]
]);
```

### Mock Config

```typescript
export function createMockConfig(overrides = {}) {
  return {
    provider: {
      name: 'openai',
      api: 'responses',
      model: 'gpt-4',
      ...overrides.provider
    },
    auth: {
      method: 'api-key',
      openai_key: 'test-key-123',
      ...overrides.auth
    }
  };
}
```

---

## Error Handling

### Expected Errors

**ConfigurationError:**
- Config file missing
- Invalid TOML format
- Required fields missing (provider, model, API key)

**AuthError:**
- API key invalid
- API key missing for selected provider

**NetworkError:**
- API call fails (network timeout, 500 error)
- Rate limit exceeded

**ValidationError:**
- Message empty
- Message too long (exceeds model limit)

### Handling in CLI

```typescript
try {
  const result = await conversation.submit(input);
} catch (err) {
  if (err instanceof ConfigurationError) {
    console.error('Configuration error:', err.message);
    console.error('Check ~/.codex/config.toml');
    process.exit(1);
  } else if (err instanceof AuthError) {
    console.error('Authentication failed:', err.message);
    console.error('Verify your API key in config');
    process.exit(1);
  } else if (err instanceof NetworkError) {
    console.error('Network error:', err.message);
    console.error('Check your connection and try again');
    process.exit(1);
  } else {
    console.error('Unexpected error:', err);
    process.exit(1);
  }
}
```

---

## Wiring Code Example

### Complete minimal wiring

```typescript
// src/cli/index.ts
import {program} from 'commander';
import {ConversationManager} from '../core/conversation-manager';
import {AuthManager} from '../core/auth';
import {loadConfig} from './config';

let manager: ConversationManager;
let activeConversation: CodexConversation | null = null;

async function getManager() {
  if (!manager) {
    const config = await loadConfig();
    const authManager = new AuthManager(/* deps */);
    manager = new ConversationManager(authManager, null);
  }
  return manager;
}

program
  .command('new')
  .action(async () => {
    const mgr = await getManager();
    const config = await loadConfig();
    const {conversationId, conversation} = await mgr.newConversation(config);
    
    activeConversation = conversation;
    console.log(`Conversation: ${conversationId.toString()}`);
  });

program
  .command('chat <message>')
  .action(async (message) => {
    if (!activeConversation) {
      console.error('No active conversation. Run: cody new');
      process.exit(1);
    }
    
    await activeConversation.submit([{type: 'text', text: message}]);
    const event = await activeConversation.nextEvent();
    
    // TODO: Render event to console
    console.log('Response:', event.msg);
  });

program.parse();
```

This is minimal wiring. Coder expands with proper error handling, display rendering, etc.

---

## Reference Code Locations

**When stuck, read these:**

- ConversationManager: `codex-ts/src/core/conversation-manager.ts` (actual signatures, how it works)
- CodexConversation: `codex-ts/src/core/codex-conversation.ts` (submit/nextEvent API)
- Codex: `codex-ts/src/core/codex/codex.ts` (spawn method, what it needs)
- Config: `codex-ts/src/core/config.ts` (full config structure)
- AuthManager: `codex-ts/src/core/auth/index.ts` (auth methods)
- ModelClient interface: `codex-ts/src/client/` (what clients implement)
- ResponseItems: `codex-ts/src/protocol/items.ts` (type definitions)

**Don't guess. Read ported code when unclear.**

---

## Key Implementation Notes

**1. ConversationManager needs AuthManager:**
Must construct AuthManager before ConversationManager. Check Phase 5 port for AuthManager construction.

**2. CodexConversation uses event loop:**
submit() → nextEvent() → nextEvent() until done. Not simple request/response. CLI must handle event stream or wrap in simpler API.

**3. Config loading:**
Use existing Config module from Phase 2 port. Don't reimplement config loading. Just read TOML and construct Config object.

**4. sessionSource parameter:**
Currently `unknown` type in port (TODO). For Phase 1: pass `null`. If Codex.spawn() requires it, investigate or ask user. Document in DECISIONS.md.

**5. Display rendering:**
CodexConversation.nextEvent() returns EventMsg union (many event types). For Phase 1, handle basic types: assistant message, error. Ignore complex events (tool calls come in Phase 2).

---

## Testing Strategy

Write mocked-service tests that verify wiring WITHOUT testing ported modules.

**What to test:**
- CLI can construct ConversationManager (wiring correct)
- CLI can call newConversation and get conversation back
- CLI can call submit() and receive events
- Multi-turn: Second submit includes first message in history

**What NOT to test:**
- Does Codex orchestration work? (Assume yes, ported code has tests)
- Does ModelClient parse SSE correctly? (Assume yes, Phase 4 tested this)
- Does conversation history work? (Assume yes, Phase 5.1 tested this)

**Test the integration, not the ported modules.**



---

TEST CONDITIONS:

# Phase 1: Test Conditions

**Test Framework:** Vitest
**Test Location:** `tests/mocked-service/phase-1-conversation-flow.test.ts`
**Mock Location:** `tests/mocks/model-client.ts`, `tests/mocks/config.ts`

---

## Test Suite: Phase 1 Conversation Flow

### Test 1: Create Conversation

**Functional description:**
User can create a new conversation. ConversationManager returns conversation with valid ID. Conversation is stored and retrievable.

**Setup:**
- Mock ModelClient (no responses needed for creation)
- Mock Config (openai, responses, api-key)

**Execute:**
- Create ConversationManager with mocks
- Call newConversation(config)

**Verify:**
- Returns NewConversation object
- conversationId is defined and valid format
- conversation object is CodexConversation instance
- Can retrieve conversation via getConversation(id)

**Implementation:** Create test file, mock setup, call newConversation, assert on return values.

---

### Test 2: Send Message and Receive Response

**Functional description:**
User can send message to conversation and receive model response. Message is submitted, model responds, response is returned.

**Setup:**
- Mock ModelClient with preset response:
  `[{type: 'message', role: 'assistant', content: [{type: 'text', text: 'Hello!'}]}]`
- Create conversation via newConversation

**Execute:**
- Call conversation.submit([{type: 'text', text: 'Test message'}])
- Call conversation.nextEvent()

**Verify:**
- Event received with assistant message
- Event contains expected text
- Mock ModelClient.sendMessage was called

**Implementation:** Setup mock with response, submit message, await event, assert on event content and mock call.

---

### Test 3: Multi-Turn Conversation Maintains History

**Functional description:**
User can have multi-turn conversation. Each new message includes previous messages in history sent to model. Context is maintained across turns.

**Setup:**
- Mock ModelClient with two responses (one per turn)
- Create conversation

**Execute:**
- Submit first message: "First message"
- Receive first response (nextEvent)
- Submit second message: "Second message"
- Receive second response

**Verify:**
- Mock ModelClient called twice
- Second call's request includes first message in history
- History length increased after each turn
- Model sees previous context

**Implementation:** Mock tracks calls, submit twice, inspect second call's request parameter, assert history includes first message.

---

### Test 4: Error Handling - Invalid Config

**Functional description:**
CLI handles invalid configuration gracefully. Error message clear and actionable.

**Setup:**
- Mock Config with missing required field (e.g., no API key)

**Execute:**
- Attempt to create conversation with invalid config

**Verify:**
- Throws ConfigurationError
- Error message describes problem
- No conversation created

**Implementation:** Create invalid config mock, expect(newConversation).rejects.toThrow(ConfigurationError).

---

### Test 5: Error Handling - No Active Conversation

**Functional description:**
If user tries to chat without creating conversation first, clear error shown.

**Setup:**
- ConversationManager with no conversations

**Execute:**
- Attempt to get conversation that doesn't exist

**Verify:**
- getConversation returns undefined
- CLI should check this and show error

**Implementation:** Call getConversation with non-existent ID, assert undefined, CLI handles gracefully.

---

## Mock Strategy

**Mock External Dependencies:**
- ModelClient (all API calls) → returns preset ResponseItems
- Filesystem (config reading) → returns test config
- Network → no real network calls

**Don't Mock:**
- ConversationManager (we're testing this integration)
- CodexConversation (part of integration)
- Codex orchestration (assume works from port)

**Test Boundary:**
CLI layer integration with ConversationManager. Everything below manager assumed working (ported code tested).

---

## Test Setup Pattern

```typescript
import {describe, it, expect, beforeEach, vi} from 'vitest';
import {ConversationManager} from '../../src/core/conversation-manager';
import {createMockClient} from '../mocks/model-client';
import {createMockConfig} from '../mocks/config';

describe('Phase 1: Conversation Flow', () => {
  let mockClient;
  let mockConfig;
  let manager;

  beforeEach(() => {
    mockClient = createMockClient([/* responses */]);
    mockConfig = createMockConfig();
    manager = new ConversationManager(/* deps with mocks */);
  });

  it('test name', async () => {
    // Test implementation
  });
});
```

---

## Success Criteria

All 5 tests pass. Mocked-service test file exists and runs. No real API calls made during tests. Tests run fast (<1 second). Tests deterministic (same result every run).


---

TASKS (update source/checklist.md as you work):

# Phase 1: Basic Chat Flow - Task Checklist

**Phase:** 1 - Basic Chat Flow
**Status:** Not Started
**Estimated Code:** ~400 lines (CLI ~300, tests ~100)

---

## Setup

- [ ] Create CLI directory structure (src/cli/)
- [ ] Install Commander.js: `npm install commander`
- [ ] Create CLI entry point: src/cli/index.ts
- [ ] Add package.json bin entry for "cody" command

---

## Config Loading

- [ ] Create src/cli/config.ts
- [ ] Implement loadConfig() - reads ~/.codex/config.toml
- [ ] Handle missing config (use defaults or error)
- [ ] Extract provider, model, auth from config
- [ ] Test: Config loads correctly

---

## ModelClient Construction

- [ ] Create src/cli/client-factory.ts
- [ ] Implement createModelClient(config)
- [ ] Switch on config.provider (openai only for Phase 1)
- [ ] Switch on config.api (responses only for Phase 1)
- [ ] Construct ResponsesClient with auth from config
- [ ] Return ModelClient interface
- [ ] Test: Client created with correct config

---

## ConversationManager Integration

- [ ] Import ConversationManager from core
- [ ] Create manager instance (inject ModelClient)
- [ ] Store manager reference (singleton or in CLI state)
- [ ] Verify: Can call manager.createConversation()

---

## CLI Commands

### cody new

- [ ] Create src/cli/commands/new.ts
- [ ] Implement newCommand handler
- [ ] Call manager.createConversation(config)
- [ ] Display conversation ID to user
- [ ] Handle errors (config invalid, client creation fails)
- [ ] Test manually: cody new → shows conversation ID

### cody chat

- [ ] Create src/cli/commands/chat.ts
- [ ] Implement chatCommand handler
- [ ] Parse message argument
- [ ] Get active conversation (from manager or stored ID)
- [ ] Call conversation.sendMessage(message)
- [ ] Pass response to display renderer
- [ ] Handle errors (no active conversation, API failure)
- [ ] Test manually: cody chat "Hello" → shows response

---

## Display Renderer

- [ ] Create src/cli/display.ts
- [ ] Implement renderResponse(items: ResponseItems[])
- [ ] Extract text content from ResponseItems
- [ ] Print to console (batch mode for Phase 1)
- [ ] Format nicely (e.g., "Assistant: [message]")
- [ ] Test: Response displays clearly

---

## Mocked-Service Tests (TDD - Write These FIRST)

- [ ] Create tests/mocked-service/ directory
- [ ] Create tests/mocks/ directory
- [ ] Create tests/mocks/model-client.ts
  - [ ] Implement createMockClient(responses)
  - [ ] Mock returns preset ResponseItems
  - [ ] No real API calls
- [ ] Create tests/mocks/config.ts
  - [ ] Mock config with test values
- [ ] Create tests/mocked-service/phase-1-conversation-flow.test.ts
- [ ] Test: Create conversation with mocked client
  - [ ] Conversation has ID
  - [ ] Conversation stored in manager
- [ ] Test: Send message with mocked client
  - [ ] Mock client called with message
  - [ ] ResponseItems returned
  - [ ] Response has expected structure
- [ ] Test: Multi-turn conversation
  - [ ] Send message 1
  - [ ] Send message 2
  - [ ] Verify: Second call includes first message in history
  - [ ] History maintained correctly
- [ ] All mocked-service tests passing

---

## Functional Verification (Manual CLI Testing)

- [ ] Test: cody new with valid config → conversation ID displayed
- [ ] Test: cody chat "Hello" → response from OpenAI displayed
- [ ] Test: cody chat "What did I just say?" → model has context
- [ ] Test: Error cases (missing config, invalid API key)
- [ ] Verify: Multi-turn chat works, context maintained

---

## Quality Gates

- [ ] Run: npm run format → no changes (already formatted)
- [ ] Run: npm run lint → 0 errors (warnings OK)
- [ ] Run: npx tsc --noEmit → 0 errors
- [ ] Run: npm test
  - [ ] Mocked-service tests: phase-1-conversation-flow.test.ts passing
  - [ ] Unit tests: 1,876+ baseline maintained
  - [ ] No skipped tests
- [ ] Combined check: npm run format && npm run lint && npx tsc --noEmit && npm test
  - [ ] All pass in sequence

---

## Documentation

- [ ] Update DECISIONS.md with key choices made
- [ ] Review: All decisions have rationale
- [ ] Verify: CHECKLIST complete (all tasks checked)

---

## Final

- [ ] All tasks above complete
- [ ] All quality gates passed
- [ ] Functional verification successful
- [ ] Code committed and pushed
- [ ] Phase 1 ready for verification stages


---

STANDARDS:

See docs/core/dev-standards.md for complete coding standards.
See docs/core/contract-testing-tdd-philosophy.md for testing approach.

Key requirements:
- TypeScript strict mode, no any types
- ESLint 0 errors
- Prettier formatted
- Mocked-service tests at library boundaries
- Mock all external dependencies


---

EXECUTION WORKFLOW:

EXECUTION WORKFLOW:

1. Read all reference documents (project context, phase design, standards, test conditions)
2. Review checklist (understand all tasks)
3. Write mocked-service tests FIRST (TDD):
   - Create test file based on test-conditions.md
   - Implement mocks (ModelClient, Config, etc.)
   - Write tests for each functional condition
   - Run tests (should fail - nothing implemented)
4. Implement code to pass tests:
   - Create files listed in checklist
   - Follow design.md implementation specifics
   - Run tests after each component
   - Iterate until all tests green
5. Manual functional testing:
   - Follow manual-test-script.md
   - Execute each test case
   - Verify expected behavior
   - Document any issues
6. Quality verification:
   - Run: npm run format (fix formatting)
   - Run: npm run lint (fix errors)
   - Run: npx tsc --noEmit (fix type errors)
   - Run: npm test (verify all pass)
   - Run combined: npm run format && npm run lint && npx tsc --noEmit && npm test
   - All must pass before proceeding
7. Update artifacts:
   - Update checklist.md (check off completed tasks)
   - Update decisions.md (log implementation choices with rationale)
8. Final verification:
   - All checklist items checked
   - All quality gates pass
   - Manual tests successful
   - Decisions documented
9. Commit and push
10. Report completion, ready for quality verifier and code reviewer

DO NOT declare phase complete until all steps verified.


---

MANUAL VERIFICATION:

# Phase 1: Manual Test Script

**Purpose:** Verify functional requirements through actual CLI usage
**Prerequisites:** Phase 1 code complete, all automated tests passing
**Duration:** ~5 minutes

---

## Setup

1. Build CLI:
   ```bash
   cd codex-ts
   npm run build
   ```

2. Ensure config exists at `~/.codex/config.toml` with valid OpenAI API key:
   ```toml
   [provider]
   name = "openai"
   api = "responses"
   model = "gpt-4o-mini"

   [auth]
   method = "api-key"
   openai_key = "sk-proj-..."
   ```

3. Make CLI available:
   ```bash
   npm link
   # or
   node src/cli/index.js
   ```

---

## Test 1: Create Conversation

**Execute:**
```bash
cody new
```

**Expected output:**
```
Created conversation: conv_[some-id]
```

**Verify:**
- ✅ Conversation ID displayed
- ✅ No errors shown
- ✅ Command completes successfully

**If fails:** Check error message. Verify config file exists and has valid API key.

---

## Test 2: Send First Message

**Execute:**
```bash
cody chat "Hello, can you hear me?"
```

**Expected output:**
```
Assistant: Yes, I can hear you! How can I help you today?
```
(Exact response varies, but should be coherent assistant message)

**Verify:**
- ✅ Response from model displayed
- ✅ Response is relevant to message sent
- ✅ No errors or warnings
- ✅ Command completes and returns to prompt

**If fails:** Check if conversation was created first. Verify API key is valid. Check network connection.

---

## Test 3: Multi-Turn Conversation (Context Maintained)

**Execute:**
```bash
cody chat "I just said hello. What did I say?"
```

**Expected output:**
```
Assistant: You said "Hello, can you hear me?"
```
(Should reference previous message, proving context maintained)

**Verify:**
- ✅ Model references previous turn
- ✅ Context from first message present
- ✅ Response coherent and contextual
- ✅ History working across turns

**If fails:** History not being maintained. Check Session history implementation. Verify conversation isn't being recreated between commands.

---

## Test 4: Multiple Messages in Sequence

**Execute:**
```bash
cody chat "My name is Alex"
cody chat "What is my name?"
cody chat "Tell me a short joke"
cody chat "What was the joke about?"
```

**Expected behavior:**
- Second message: Model says "Alex" or "Your name is Alex"
- Fourth message: Model references the joke topic

**Verify:**
- ✅ All 4 messages get responses
- ✅ Context maintained throughout
- ✅ Model can reference information from any previous turn
- ✅ No degradation or errors over multiple turns

---

## Test 5: Error Cases

**Test 5a: No active conversation**

Execute (without running `cody new` first):
```bash
cody chat "This should fail"
```

Expected: Clear error message like "No active conversation. Run: cody new"

**Test 5b: Invalid config**

Temporarily rename ~/.codex/config.toml, then:
```bash
cody new
```

Expected: Configuration error with helpful message pointing to config file.

Restore config after test.

---

## Success Checklist

After completing all tests:

- [ ] Test 1: Can create conversation
- [ ] Test 2: Can send message and receive response
- [ ] Test 3: Multi-turn context maintained
- [ ] Test 4: Multiple sequential turns work
- [ ] Test 5a: Error handling works (no conversation)
- [ ] Test 5b: Error handling works (invalid config)

**All tests pass:** Phase 1 functional requirements verified.

**Any test fails:** Document failure, investigate, fix before declaring phase complete.

---

## Notes for Verification

User (project owner) runs these tests manually. Take notes on:
- Does CLI feel responsive?
- Are error messages clear?
- Is UX acceptable?
- Any rough edges to polish in Phase 7?

This is qualitative validation beyond automated tests. User experience matters.


---

FINAL QUALITY CHECK:

Before declaring phase complete:

Run: npm run format && npm run lint && npx tsc --noEmit && npm test

ALL must pass. Document results.
Update checklist.md and decisions.md.
Commit and push.
Ready for verification stages.

===== END CODER PROMPT =====
