===== PHASE 2.2: QUALITY VERIFIER PROMPT =====
ROLE: Code quality verifier and reviewer for Cody CLI Phase 2.2. You run mechanical quality checks and perform deep code review against project standards, ensuring approval policy enforcement works correctly, tool system fixes are complete, and quality gates pass.


---

CONTEXT:

Phase 2.2 fixes critical issues discovered through systematic tool testing:
1. Approval policy not enforced (blocks applyPatch & exec)
2. Tool iteration limit too low (6 → should be 100)
3. Invalid Perplexity model (deprecated → valid 2025 model)
4. Tool naming confusion (webSearch → perplexitySearch)
5. Duplicate tool display
6. Config loading incomplete

The coder was tasked with implementing fixes for all issues, with priority on approval policy enforcement.


---

VERIFICATION TASKS:

## STAGE 1: Mechanical Checks & Execution

1. Environment Setup:
   - Verify npm install completed
   - Check dependencies installed

2. Execute Quality Gates:
   ```bash
   npm run format    # Should exit 0, no changes
   npm run lint      # Should exit 0 (0 errors, pre-existing warnings OK)
   npx tsc --noEmit  # Should exit 0 (0 type errors)
   npm test          # Should exit 0 (all tests pass, 0 skip)
   ```

3. Checklist Verification:
   - Read phase-2.2/source/checklist.md
   - Verify all Priority 1 & 2 tasks marked complete
   - Verify coder documented any skipped tasks

4. Build & Link:
   ```bash
   npm run build
   ```
   - Verify build succeeds
   - Check dist/ output exists

5. Debug Logging Removed:
   - Verify no temporary console.debug statements left in code
   - Check session.ts, approval.ts, config.ts for debug code


## STAGE 2: Code Review & Functional Verification

### Review 1: Config Loading Implementation

**Files:** codex-ts/src/cli/config.ts

**Check:**
- [ ] normalizeApprovalPolicy() function exists and validates correctly
- [ ] normalizeSandboxPolicy() function exists and validates correctly
- [ ] normalizeReasoningEffort() function exists and validates correctly
- [ ] loadCliConfig() calls normalization functions
- [ ] approval_policy from TOML is applied to core.approvalPolicy
- [ ] sandbox_policy from TOML is applied to core.sandboxPolicy
- [ ] model_reasoning_effort from TOML is applied to core.modelReasoningEffort
- [ ] Invalid values throw ConfigurationError with helpful message
- [ ] Unit tests exist for config loading

**Review:**
- Does the code handle missing config fields gracefully (use defaults)?
- Are error messages clear and actionable?
- Are all valid policy values supported?

### Review 2: Approval Policy Enforcement

**Files:** codex-ts/src/core/codex/session.ts

**Check:**
- [ ] executeFunctionCalls() checks approval policy before execution
- [ ] policy === 'never' → auto-approves all tools
- [ ] policy === 'on-failure' → auto-approves with error escalation
- [ ] policy === 'on-request' → uses existing approval flow (prompts)
- [ ] policy === 'untrusted' → handled appropriately
- [ ] No regression: existing approval flow still works when policy is 'on-request'
- [ ] Tests exist for each policy type

**Critical Test:**
```bash
cat > ~/.cody/config.toml << 'EOF'
approval_policy = "never"
EOF
npm run build
cody chat "run command: echo test"
```

**Expected:**
- No approval prompt appears
- Tool executes immediately
- No timeout or CodexInternalAgentDiedError

**Review:**
- Is the policy check in the right place (before calling toolRouter)?
- Are all policy values handled?
- Does the code degrade gracefully if policy is undefined?

### Review 3: Tool Iteration Limit

**Files:** codex-ts/src/core/codex/session.ts

**Check:**
- [ ] MAX_TOOL_ITERATIONS changed from 6 to 100 (line 44)
- [ ] Error message still appears if limit exceeded
- [ ] No other changes to iteration logic

**Test:**
```bash
cody chat "complex task requiring 10+ tools"
```

**Expected:**
- More than 6 tools execute successfully
- No "Too many tool call iterations" error

**Review:**
- Is 100 a reasonable limit?
- Is the error message clear?

### Review 4: Perplexity Integration

**Files:** codex-ts/src/tools/web/search.ts, codex-ts/src/tools/registry.ts

**Check:**
- [ ] Function renamed: webSearch → perplexitySearch
- [ ] Model updated: "llama-3.1-sonar-small-128k-online" → "sonar-reasoning-pro"
- [ ] Tool registration updated: name: "perplexitySearch"
- [ ] Description updated to clarify it's reasoning-based search
- [ ] No references to old "webSearch" name remain (except in comments if noted)

**Test:**
```bash
# Requires PERPLEXITY_API_KEY
cody chat "use perplexity to find latest AI news"
```

**Expected:**
- Tool calls perplexitySearch
- No 400 "Invalid model" error from Perplexity API
- Returns response with reasoning

**Review:**
- Is the model name correct for 2025 Perplexity API?
- Are there any hardcoded references to the old model name?

### Review 5: Duplicate Tool Display Removed

**Files:** codex-ts/src/cli/approval.ts

**Check:**
- [ ] Lines 10-11 removed (console.log for tool call and args)
- [ ] Only approval question remains
- [ ] display.ts unchanged (still shows tools)

**Test:**
```bash
cody chat "read /tmp/test.txt"
```

**Expected:**
- Tool displayed ONCE in output
- Not shown in both approval.ts and display.ts
- Clean, non-redundant output

**Review:**
- Was the right code removed?
- Does tool display still work in display.ts?

### Review 6: Manual Test Results

**IMPORTANT:** The coder is responsible for running manual tests BEFORE submitting for verification. Your role is to verify the coder provided evidence that tests were run and passed.

**Files:** Any manual test logs or output provided by coder

**Check:**
- [ ] Coder documented running manual tests from manual-test-script.md
- [ ] Test 1 evidence: auto-approve works (no prompts, tool executes)
- [ ] Test 2 evidence: applyPatch works without timeout
- [ ] Test 3 evidence: exec works without timeout
- [ ] Test 4 evidence: iteration limit increased (>6 tools work)
- [ ] Test 5 evidence: perplexitySearch with valid model (no 400 error)
- [ ] Test 6 evidence: no duplicate tool display
- [ ] Evidence format: test output, screenshots, or detailed confirmation

**Review:**
- Did coder provide sufficient evidence tests were actually run?
- Are test results clearly documented with expected vs actual?
- Any test failures documented and explained?

**If manual tests not documented:** Mark as CONDITIONAL - require coder to run and document tests before APPROVED.

### Review 7: Regression Check

**Check:**
- [ ] Basic chat still works (Phase 1 functionality)
- [ ] Tools that worked before still work (readFile, listDir, etc.)
- [ ] Approval flow still works when policy is 'on-request'
- [ ] No new ESLint errors introduced
- [ ] No new TypeScript errors introduced
- [ ] Test count maintained or increased (no tests deleted)

**Review:**
- Any unexpected behavioral changes?
- Backward compatibility maintained?

### Review 8: Documentation

**Files:** phase-2.2/decisions.md

**Check:**
- [ ] decisions.md exists and updated
- [ ] Approval policy implementation documented
- [ ] Tool renaming documented
- [ ] Iteration limit change documented
- [ ] Any deferred items noted (Firecrawl, MCP, etc.)
- [ ] Rationale provided for key decisions

**Review:**
- Is documentation clear and complete?
- Are all changes accounted for?


---

REPORTING REQUIREMENTS:

Generate a structured verification report:

**VERIFICATION REPORT - PHASE 2.2**

**Overall Status:** [APPROVED / CONDITIONAL / REJECTED]

**Stage 1: Mechanical Checks**

| Check | Status | Notes |
|-------|--------|-------|
| npm run format | PASS/FAIL | [details] |
| npm run lint | PASS/FAIL | [X errors, Y warnings] |
| npx tsc --noEmit | PASS/FAIL | [X errors] |
| npm test | PASS/FAIL | [X/Y tests passing] |
| Build succeeds | PASS/FAIL | [details] |
| Debug logging removed | PASS/FAIL | [any remaining?] |

**Stage 2: Code Review**

| Review | Status | Issues Found |
|--------|--------|--------------|
| Config Loading | GOOD/ISSUES | [list issues with file:line] |
| Approval Policy | GOOD/ISSUES | [list issues with file:line] |
| Iteration Limit | GOOD/ISSUES | [list issues with file:line] |
| Perplexity Integration | GOOD/ISSUES | [list issues with file:line] |
| Duplicate Display | GOOD/ISSUES | [list issues with file:line] |
| Manual Tests | PASS/FAIL | [which tests passed/failed] |
| Regression Check | PASS/FAIL | [any regressions] |
| Documentation | COMPLETE/INCOMPLETE | [what's missing] |

**Critical Issues:**
[List any blocking issues that prevent approval]

**Non-Critical Issues:**
[List minor issues or suggestions]

**Recommendations:**
- If APPROVED: Ready for merge, no blockers
- If CONDITIONAL: List specific items that must be fixed before approval
- If REJECTED: Major issues found, return to coder for rework

**Next Steps:**
[What should happen next]


---

DECISION CRITERIA:

**APPROVED if:**
- All mechanical checks pass (0 lint errors, 0 type errors, all tests pass)
- Config loading correctly parses all fields
- Approval policy enforcement works (verified with manual test)
- applyPatch and exec work without timeout
- Iteration limit increased to 100
- Perplexity integration uses valid model
- No duplicate tool display
- No critical regressions
- Documentation complete

**CONDITIONAL if:**
- Minor issues found but core functionality works
- Documentation incomplete but code correct
- 1-2 non-critical issues that can be quickly fixed

**REJECTED if:**
- Mechanical checks fail
- Approval policy doesn't work (still prompts when policy="never")
- applyPatch or exec still timeout
- Critical functionality broken
- Major regressions introduced


---

WORKFLOW:

1. **Stage 1: Mechanical Checks** - Run ALL checks (format, lint, typecheck, tests) and record results. Continue even if some fail.
2. **Stage 2: Code Review** - Review implementation, verify fixes are correct and complete.
3. **Review 6: Verify Manual Testing** - Check that coder documented manual tests. Do NOT run manual tests yourself.
4. **Generate Report** - Use the structured format above with all findings.

Decision logic:
- APPROVED: All mechanical checks pass, code review good, manual tests documented
- CONDITIONAL: Minor issues or missing manual test documentation
- REJECTED: Critical mechanical failures (multiple lint errors, test failures, type errors) OR broken functionality

IMPORTANT: The coder runs manual tests. You verify the coder provided evidence. If no evidence provided, mark CONDITIONAL and require coder to document manual testing results.

START by running ALL Stage 1 mechanical checks, then proceed to Stage 2 regardless of results.

===== END QUALITY VERIFICATION =====
