===== PHASE 5: CODER PROMPT =====
ROLE: Senior TypeScript developer implementing phases of the Cody CLI integration project. You write clean, tested code following TDD principles with mocked-service tests at library boundaries.


---

PRODUCT:

**Cody** is a command-line interface for the Codex TypeScript library. It provides multi-provider LLM agent capabilities supporting OpenAI (Responses, Chat) and Anthropic (Messages) APIs with tool execution, conversation persistence, and structured tool calling. Built as a TypeScript port of OpenAI's Rust-based Codex CLI, Cody serves as both a standalone CLI tool and reference implementation for the @openai/codex-core library.


---

PROJECT CONTEXT:

# Project 02: UI Integration & Library Definition

## What We're Building

Project 02 integrates all ported Codex modules (Phases 1-6) into a working command-line interface called **Cody** and defines the library API surface for @openai/codex-core. This project validates the Rust → TypeScript port by wiring protocol, configuration, persistence, execution, client, tools, and orchestration layers into complete conversation flows.

## Why It Matters

The port is functionally complete but untested as an integrated system. Individual modules have unit tests, but we haven't verified end-to-end workflows. This project proves the port works, exposes integration issues, and establishes the library interface that external developers will use.

## Project Success Criteria

By project completion:
- User can create conversations, send messages, receive responses (all providers: OpenAI Responses/Chat, Anthropic Messages)
- All auth methods work (API keys, ChatGPT OAuth, Claude OAuth)
- Tools execute with approval flow
- Conversations persist and resume (JSONL format)
- MCP integration functional
- Library API documented (public exports, usage examples)
- REST API designed (optional implementation)
- Zero-error quality baseline maintained (0 TS errors, 0 ESLint errors, all tests passing)

## Dependencies

- Phase 6 complete (75 modules ported, 1,876 tests passing)
- Phase 5.2 complete (quality baseline clean)
- API keys: OpenAI, Anthropic, OpenRouter
- OAuth tokens: Read from ~/.codex (ChatGPT), ~/.claude (Claude)

## Scope

**In scope:** CLI commands, provider integration (3 APIs), auth methods (4 total), tool execution, persistence/resume, library API docs, REST API spec

**Non-scope:** Script harness (Project 03), memory innovations (Projects 04-06), rich TUI, additional tools, performance optimization, production hardening


---

PHASE 5 TECHNICAL DESIGN:

# Phase 5: Technical Design

**Phase:** Persistence & Resume + History Compression
**Goal:** Wire RolloutRecorder for automatic JSONL persistence, implement resume flow, port Codex compact algorithm for long conversation support

---

## Integration Overview

(From TECH-APPROACH Section 6)

Phase 5 wires the RolloutRecorder from Phase 2 port into the conversation flow. Conversations now save to JSONL files in ~/.codex/conversations/ as they progress. Each turn appends to the rollout file. CLI adds commands for listing saved conversations and resuming from JSONL. Resume reconstructs conversation state from rollout, loads into ConversationManager, user continues where they left off.

The integration has two parts: recording (save as conversation progresses) and resuming (load from JSONL). Recording happens automatically during conversation—after each turn, Codex flushes state to RolloutRecorder, recorder appends to JSONL. Resuming requires reading JSONL, parsing rollout items, reconstructing conversation history, initializing Session with that history, creating Conversation wrapper. ConversationManager.resumeConversation() handles this orchestration.

Additionally, Phase 5 ports the compact algorithm from Rust (codex-rs/core/src/codex/compact.rs, ~450 lines). This is critical Codex functionality that was missed in the initial port. Without it, long conversations hit context limits and fail. Compact detects when history approaches token threshold, sends summarization prompt to LLM, receives summary, rebuilds history keeping initial context + selected recent user messages + summary + GhostSnapshots. Handles retry with backoff if compression insufficient.

Testing mocks filesystem for JSONL reads/writes and mocks LLM calls for compression. In-memory buffer simulates file. Test conversation creates turns, recorder "writes" to buffer, resume reads from buffer, verify conversation continues. Test compact triggers when history large, verify summarization called, history rebuilt correctly.

### Phase 5 Target State

```
User runs: cody list
           cody resume conv_abc123

┌──────────────────────────────────┐
│  CLI (Phase 1-4 + NEW)           │
│  ┌──────┐  ┌────────────┐       │
│  │ list │  │  resume    │       │
│  │(NEW) │  │ <conv-id>  │ (NEW) │
│  └──┬───┘  └─────┬──────┘       │
│     │            │               │
│     └────────────┘               │
│           ▼                      │
│    List/Resume Handler           │
└───────────┬──────────────────────┘
            ▼
┌──────────────────────────────────┐
│  ConversationManager             │
│  ┌────────────────────────────┐  │
│  │ resumeConversation() (NEW) │  │
│  │  - Read JSONL              │  │
│  │  - Reconstruct history     │  │
│  │  - Initialize Session      │  │
│  └────────────────────────────┘  │
└──────────┬───────────────────────┘
           ▼
┌──────────────────────────────────┐
│  RolloutRecorder (ACTIVATED)     │
│  ┌────────────────────────────┐  │
│  │  First real use            │  │
│  │  - Append to JSONL         │  │
│  │  - Parse rollout items     │  │
│  └────────────────────────────┘  │
└──────────┬───────────────────────┘
           ▼
┌──────────────────────────────────┐
│  Compact Module (NEW)            │
│  ┌────────────────────────────┐  │
│  │  History compression       │  │
│  │  - Detect threshold        │  │
│  │  - Summarize via LLM       │  │
│  │  │  - Rebuild history       │  │
│  └────────────────────────────┘  │
└──────────┬───────────────────────┘
           ▼
    ~/.codex/conversations/
    conv_abc123.jsonl
       (MOCKED in tests)
```

**Highlighted:** list and resume commands (NEW), resumeConversation() method (NEW), RolloutRecorder (ACTIVATED from port), Compact module (NEW, ported from Rust), JSONL persistence.

---

## Actual Signatures (from ported code)

### RolloutRecorder

Location: `codex-ts/src/core/rollout/recorder.ts` (or similar from Phase 2 port)

```typescript
class RolloutRecorder {
  constructor(baseDir: string)
  
  async appendTurn(conversationId: string, turn: RolloutTurn): Promise<void>
  // Appends turn to JSONL file (one line per turn)
  
  async readRollout(conversationId: string): Promise<RolloutTurn[]>
  // Reads JSONL file, parses into array of turns
  
  async list(): Promise<ConversationMetadata[]>
  // Lists all conversations in baseDir
}

interface RolloutTurn {
  timestamp: number;
  items: ResponseItem[];
  metadata?: {
    provider?: string;
    model?: string;
    tokens?: number;
  };
}

interface ConversationMetadata {
  id: string;
  updatedAt: number;
  provider?: string;
  model?: string;
}
```

### ConversationManager Extensions

```typescript
class ConversationManager {
  // Existing from Phase 1
  async newConversation(config: Config): Promise<NewConversation>
  async getConversation(id: ConversationId): Promise<CodexConversation | undefined>
  
  // NEW for Phase 5
  async resumeConversation(id: ConversationId): Promise<CodexConversation>
  // Loads from JSONL, reconstructs history, returns conversation
  
  async listConversations(): Promise<ConversationMetadata[]>
  // Lists all saved conversations
}
```

### Compact Module (NEW - port from Rust)

Location: Create `codex-ts/src/core/codex/compact.ts`

```typescript
// Main entry point
export async function runCompactTask(
  session: Session,
  turnContext: TurnContext,
  input: UserInput[]
): Promise<void>

// Core algorithm
async function compactHistory(
  session: Session,
  turnContext: TurnContext
): Promise<void> {
  // 1. Get current history
  const history = session.getHistory();
  
  // 2. Check if compression needed
  if (!needsCompaction(history, turnContext.modelContextWindow)) {
    return;
  }
  
  // 3. Send summarization prompt to LLM
  const summary = await getSummary(session, turnContext, history);
  
  // 4. Collect recent user messages
  const userMessages = collectUserMessages(history);
  
  // 5. Build compacted history
  const newHistory = buildCompactedHistory(
    session.getInitialContext(),
    userMessages,
    summary
  );
  
  // 6. Preserve GhostSnapshots
  const ghosts = history.filter(item => item.type === 'ghost_snapshot');
  newHistory.push(...ghosts);
  
  // 7. Replace session history
  session.replaceHistory(newHistory);
  
  // 8. Persist CompactedItem to rollout
  await recorder.appendCompacted(summary);
}

// Helper: Check if history too large
function needsCompaction(
  history: ResponseItem[],
  contextWindow: number
): boolean {
  const tokens = countTokens(history);
  const threshold = contextWindow * 0.8; // 80% of context
  return tokens > threshold;
}

// Helper: Build new history after compression
function buildCompactedHistory(
  initialContext: ResponseItem[],
  userMessages: string[],
  summary: string
): ResponseItem[] {
  const history = [...initialContext]; // System prompt, base instructions
  
  // Add selected recent user messages (up to token limit)
  const selected = selectRecentMessages(userMessages, MAX_USER_MESSAGE_TOKENS);
  for (const msg of selected) {
    history.push({
      type: 'message',
      role: 'user',
      content: [{type: 'text', text: msg}]
    });
  }
  
  // Add summary as user message
  history.push({
    type: 'message',
    role: 'user',
    content: [{type: 'text', text: summary}]
  });
  
  return history;
}

// Helper: Select recent user messages within token budget
function selectRecentMessages(
  messages: string[],
  maxTokens: number
): string[] {
  const selected = [];
  let remaining = maxTokens;
  
  // Work backwards (most recent first)
  for (let i = messages.length - 1; i >= 0; i--) {
    const msg = messages[i];
    const tokens = countTokens(msg);
    
    if (tokens <= remaining) {
      selected.unshift(msg); // Add to front (preserve order)
      remaining -= tokens;
    } else if (remaining > 0) {
      // Truncate middle of message (keep head + tail)
      const truncated = truncateMiddle(msg, remaining);
      selected.unshift(truncated);
      break;
    } else {
      break;
    }
  }
  
  return selected;
}
```

**Constants from Rust:**
- COMPACT_USER_MESSAGE_MAX_TOKENS = 20,000
- Threshold = 80% of model context window
- Max retries = provider-specific (usually 3-6)

---

## Technical Deltas

**New code (CLI layer):**
- src/cli/commands/list.ts: List saved conversations (~50 lines)
- src/cli/commands/resume.ts: Resume conversation by ID (~60 lines)

**New code (integration):**
- src/core/conversation-manager.ts: resumeConversation() method (~80 lines)
- src/core/conversation-manager.ts: listConversations() method (~30 lines)

**New code (persistence):**
- src/core/codex/session.ts: Auto-flush to RolloutRecorder after each turn (~20 lines modification)

**New code (compression - port from Rust):**
- src/core/codex/compact.ts: Complete compact algorithm (~450 lines)
  - runCompactTask() - main entry
  - needsCompaction() - threshold check
  - getSummary() - LLM summarization call
  - collectUserMessages() - extract user messages from history
  - buildCompactedHistory() - reconstruct history
  - selectRecentMessages() - token budget allocation
  - truncateMiddle() - message truncation helper

**New code (testing):**
- tests/mocked-service/phase-5-persistence.test.ts: Save/resume/compact tests (~150 lines)
- tests/mocks/rollout-recorder.ts: In-memory JSONL mock (~60 lines)
- tests/mocks/model-client.ts: Extend to mock summarization calls (~20 lines addition)

**Estimated new code:** ~920 lines total
- CLI commands: ~110 lines
- Resume logic: ~110 lines
- Compact module: ~450 lines
- Mocked-service tests: ~150 lines
- Mocks: ~100 lines

---

## Persistence Cycle (Critical Path)

**Auto-save during conversation:**

After each turn completes (model response received, tool calls executed, results returned), Session has complete ResponseItems array for the turn. Session calls RolloutRecorder.appendTurn() with conversation ID and turn data. Recorder serializes ResponseItems to JSON (one line), appends to ~/.codex/conversations/{conversationId}.jsonl. File grows as conversation progresses. User doesn't trigger save manually—it's automatic after every turn.

**Why automatic:** Prevents data loss if CLI crashes or user exits without explicit save. Every turn is immediately persisted. Resume always has latest state.

**JSONL format:**
```jsonl
{"timestamp":1699564800,"items":[{"type":"message","role":"user","content":[{"type":"text","text":"Hello"}]}],"metadata":{"provider":"openai","model":"gpt-4"}}
{"timestamp":1699564805,"items":[{"type":"message","role":"assistant","content":[{"type":"text","text":"Hi there!"}]}],"metadata":{"tokens":15}}
{"timestamp":1699564810,"items":[{"type":"message","role":"user","content":[{"type":"text","text":"How are you?"}]}]}
```

One JSON object per line. Each line is complete turn. File is append-only (never rewrite entire file).

**Persistence cycle steps:**
1. Turn completes → Session has ResponseItems
2. Session → recorder.appendTurn(conversationId, {timestamp, items, metadata})
3. Recorder serializes to JSON string
4. Append to file: `${baseDir}/${conversationId}.jsonl`
5. Flush to disk (ensure written)

**Resume cycle:**

User runs `cody resume conv_abc123`. CLI calls ConversationManager.resumeConversation('conv_abc123'). Manager constructs file path, calls RolloutRecorder.readRollout(). Recorder reads file line-by-line, parses each JSON line, builds array of RolloutTurn objects. Manager extracts ResponseItems from each turn, flattens into single history array. Initializes new Session with pre-loaded history. Creates CodexConversation wrapper. Returns to CLI. User continues conversation—model has full context from loaded history.

**Resume cycle steps:**
1. User: `cody resume conv_abc123`
2. CLI → manager.resumeConversation('conv_abc123')
3. Manager → recorder.readRollout('~/.codex/conversations/conv_abc123.jsonl')
4. Recorder reads file, parses lines
5. Recorder returns RolloutTurn[]
6. Manager extracts items: `turns.flatMap(t => t.items)`
7. Manager creates Session with history pre-loaded
8. Manager creates CodexConversation(session)
9. Manager returns conversation to CLI
10. CLI prints "Resumed conversation conv_abc123"
11. User can continue chatting

**Persistence Cycle Sequence:**

```mermaid
sequenceDiagram
    participant User
    participant CLI
    participant Conv as Conversation
    participant Session
    participant Recorder as RolloutRecorder
    participant FS as Filesystem

    Note over User,FS: SAVE FLOW (automatic after each turn)

    User->>CLI: cody chat "message"
    CLI->>Conv: submit(input)
    Conv->>Session: processMessage()
    Session->>Session: Send to ModelClient, receive items
    Session->>Session: Execute tools if needed
    Session->>Session: Turn complete

    Session->>Recorder: appendTurn(convId, {timestamp, items})
    Recorder->>Recorder: Serialize items to JSON
    Recorder->>FS: Append line to conv_abc123.jsonl
    FS-->>Recorder: Write complete

    Session->>Conv: return items
    Conv->>CLI: return event
    CLI->>User: Display response

    Note over User,FS: RESUME FLOW (explicit command)

    User->>CLI: cody resume conv_abc123
    CLI->>CLI: Load ConversationManager
    CLI->>Recorder: readRollout("conv_abc123")
    Recorder->>FS: Read conv_abc123.jsonl
    FS-->>Recorder: File contents (JSONL lines)
    Recorder->>Recorder: Parse each line
    Recorder-->>CLI: RolloutTurn[]
    CLI->>CLI: Extract items, create Session with history
    CLI->>CLI: Create Conversation wrapper
    CLI->>User: "Resumed conversation conv_abc123"

    User->>CLI: cody chat "continue from before"
    Note over Session: Model has full history from JSONL
```

---

## Compact Algorithm (Port from Rust)

**Source:** `codex-rs/core/src/codex/compact.rs`

**Purpose:** Prevent context window overflow in long conversations by compressing older history.

### Algorithm Overview

**Trigger:** After each turn, check if history token count > 80% of model context window.

**Process:**
1. Send summarization prompt to LLM with current history
2. LLM returns summary of older conversation
3. Collect recent user messages (for context continuity)
4. Build new history: initial context + selected user messages + summary + GhostSnapshots
5. Replace session history with compacted version
6. Persist CompactedItem to rollout (records that compression happened)

**Retry logic:** If compacted history still too large (rare), remove oldest items and retry. Max retries based on provider settings.

### Detailed Steps

**Step 1: Threshold Check**

```typescript
function needsCompaction(history: ResponseItem[], contextWindow: number): boolean {
  const tokens = countHistoryTokens(history);
  const threshold = Math.floor(contextWindow * 0.8);
  return tokens > threshold;
}
```

Called after each turn. If true, trigger compact.

**Step 2: Summarization Prompt**

```typescript
const SUMMARIZATION_PROMPT = `
Please provide a concise summary of the conversation so far.
Focus on:
- Key decisions made
- Important information shared
- Current state of work
- Any unresolved issues

Keep summary under 500 tokens.
`;

async function getSummary(
  session: Session,
  history: ResponseItem[]
): Promise<string> {
  // Build prompt with current history
  const prompt = {
    input: history,
    systemPrompt: SUMMARIZATION_PROMPT
  };
  
  // Send to LLM (same ModelClient conversation uses)
  const response = await session.client.sendMessage(prompt);
  
  // Extract summary text from response
  const summaryText = extractTextFromResponse(response);
  
  return summaryText || '(no summary available)';
}
```

**This makes a real LLM call.** Uses same client/model as conversation. Costs tokens but prevents context overflow.

**Step 3: Collect User Messages**

```typescript
function collectUserMessages(history: ResponseItem[]): string[] {
  return history
    .filter(item => item.type === 'message' && item.role === 'user')
    .map(item => {
      // Extract text from content array
      const textItems = item.content.filter(c => c.type === 'text');
      return textItems.map(c => c.text).join('\n');
    })
    .filter(text => text.length > 0);
}
```

**Why collect user messages:** Preserve user's actual words for context. Summary is assistant's perspective. User messages are ground truth.

**Step 4: Select Recent Messages**

```typescript
const MAX_USER_MESSAGE_TOKENS = 20_000;

function selectRecentMessages(
  messages: string[],
  maxTokens: number = MAX_USER_MESSAGE_TOKENS
): string[] {
  const selected: string[] = [];
  let remaining = maxTokens;
  
  // Work backwards (most recent first)
  for (let i = messages.length - 1; i >= 0; i--) {
    const msg = messages[i];
    const tokens = countTokens(msg);
    
    if (tokens <= remaining) {
      // Fits completely
      selected.unshift(msg);
      remaining -= tokens;
    } else if (remaining > 0) {
      // Truncate middle, keep head + tail
      const truncated = truncateMiddle(msg, remaining);
      selected.unshift(truncated);
      break; // Can't fit more
    } else {
      break; // Budget exhausted
    }
  }
  
  return selected;
}

function truncateMiddle(text: string, maxTokens: number): string {
  const tokens = countTokens(text);
  if (tokens <= maxTokens) return text;
  
  // Keep first 40% and last 40%, replace middle with "..."
  const chars = text.length;
  const keepRatio = maxTokens / tokens;
  const keepChars = Math.floor(chars * keepRatio);
  const headChars = Math.floor(keepChars * 0.4);
  const tailChars = Math.floor(keepChars * 0.4);
  
  const head = text.slice(0, headChars);
  const tail = text.slice(chars - tailChars);
  
  return `${head}\n\n[... ${chars - keepChars} characters truncated ...]\n\n${tail}`;
}
```

**Token budget:** 20k tokens for user messages. Recent messages prioritized. Older messages truncated or dropped.

**Step 5: Build Compacted History**

```typescript
function buildCompactedHistory(
  initialContext: ResponseItem[],
  userMessages: string[],
  summary: string
): ResponseItem[] {
  const history = [...initialContext]; // System prompt, base instructions
  
  // Add selected user messages
  for (const msg of userMessages) {
    history.push({
      type: 'message',
      role: 'user',
      content: [{type: 'text', text: msg}]
    });
  }
  
  // Add summary as user message
  history.push({
    type: 'message',
    role: 'user',
    content: [{
      type: 'text',
      text: `Summary of earlier conversation:\n\n${summary}`
    }]
  });
  
  return history;
}
```

**Result:** New history with structure:
1. Initial context (system prompt, etc.)
2. Selected recent user messages (up to 20k tokens)
3. Summary message (LLM-generated summary of older turns)
4. GhostSnapshots (special items preserved across compression)

**Step 6: Retry Logic**

```typescript
async function compactWithRetry(
  session: Session,
  turnContext: TurnContext
): Promise<void> {
  let history = session.getHistory();
  let truncatedCount = 0;
  const maxRetries = turnContext.client.getProvider().streamMaxRetries();
  let retries = 0;
  
  while (true) {
    try {
      // Attempt to send compacted history to LLM
      await sendCompactedHistory(session, history);
      
      // Success - notify user if we truncated
      if (truncatedCount > 0) {
        session.notifyUser(
          `Trimmed ${truncatedCount} older item(s) before compacting ` +
          `to fit context window`
        );
      }
      break;
      
    } catch (err) {
      if (err.code === 'CONTEXT_WINDOW_EXCEEDED') {
        // Still too large - remove oldest item and retry
        if (history.length > 1) {
          history = history.slice(1); // Remove first item
          truncatedCount++;
          retries = 0; // Reset retry counter
          continue;
        } else {
          // Can't compress further
          throw new Error('Cannot compact: history too large even after truncation');
        }
      }
      
      // Other errors - retry with backoff
      if (retries < maxRetries) {
        retries++;
        await sleep(backoff(retries)); // 250ms, 500ms, 1s, 2s, ...
        continue;
      } else {
        throw err; // Give up
      }
    }
  }
}
```

**Retry strategy:**
- Context exceeded → remove oldest item, try again
- Network errors → backoff and retry (up to max)
- Other errors → fail after retries exhausted

---

## Component Structure

Persistence involves RolloutRecorder for JSONL I/O, compact module for compression, Session for history management, and CLI for user commands.

```mermaid
classDiagram
    class CLI {
        +listConversations()
        +resumeConversation(id: string)
    }

    class ConversationManager {
        -recorder: RolloutRecorder
        +resumeConversation(id: string) Promise~Conversation~
        +listConversations() Promise~Metadata[]~
        -loadFromRollout(path: string)
    }

    class RolloutRecorder {
        -baseDir: string
        +appendTurn(id: string, turn: RolloutTurn) Promise~void~
        +readRollout(id: string) Promise~RolloutTurn[]~
        +list() Promise~Metadata[]~
        -serializeToJSONL(turn)
        -parseJSONL(lines)
    }

    class Session {
        -history: ResponseItem[]
        -recorder: RolloutRecorder
        +processMessage(msg) Promise~ResponseItems[]~
        +getHistory() ResponseItem[]
        +replaceHistory(items: ResponseItem[])
        -checkCompaction()
    }

    class CompactModule {
        +runCompactTask(session, context)
        +needsCompaction(history, contextWindow) boolean
        +getSummary(session, history) Promise~string~
        +buildCompactedHistory(initial, messages, summary)
        -selectRecentMessages(messages, maxTokens)
        -truncateMiddle(text, maxTokens)
    }

    CLI --> ConversationManager: resume, list
    ConversationManager --> RolloutRecorder: read/write
    Session --> RolloutRecorder: append after each turn
    Session --> CompactModule: trigger when threshold exceeded
    CompactModule --> Session: replace history
```

---

## Connection Points Detail

### Session → RolloutRecorder (Auto-Save)

**After each turn:**

Session.processMessage() completes (has ResponseItems for turn). Before returning to caller, Session calls recorder.appendTurn(). Passes conversation ID (stored in Session), turn data (timestamp, items, metadata). Recorder handles serialization and file I/O. Session doesn't wait for write completion (fire-and-forget or await based on reliability needs—document choice in DECISIONS.md).

**Wiring point:** Session constructor must receive RolloutRecorder instance. ConversationManager injects recorder when creating Session.

### Session → Compact Module (Threshold Check)

**After each turn, before returning:**

Session checks if history exceeds threshold. Calls compact.needsCompaction(history, modelContextWindow). If true, triggers compact.runCompactTask(). Compact makes LLM call (summarization), builds new history, calls session.replaceHistory(). Session continues with compacted history. Next turn uses smaller history.

**Wiring:** Compact module imported by Session. Called conditionally based on threshold. Compact is async (LLM call), so Session awaits completion before proceeding.

**User visibility:** Session can emit event or log message: "Compacting conversation history..." User sees compression happening (not silent).

### ConversationManager → RolloutRecorder (Resume)

**Resume flow:**

Manager.resumeConversation(id) calls recorder.readRollout(id). Recorder returns RolloutTurn[] (parsed from JSONL). Manager iterates turns, extracts ResponseItems, builds flat history array. Also extracts metadata (provider, model, last timestamp). Creates Session with pre-loaded history. Creates CodexConversation wrapper. Returns conversation ready for use.

**Error handling:**
- File not found → throw with message "Conversation {id} not found. Run: cody list"
- Corrupted JSONL → throw with line number and parsing error
- Empty file → throw "Conversation {id} has no turns"

### CLI → ConversationManager (List & Resume Commands)

**list command:**

Calls manager.listConversations(). Manager delegates to recorder.list(). Recorder scans baseDir, reads metadata from each .jsonl file (first line has conversation metadata). Returns array of {id, updatedAt, provider, model}. CLI formats as table, prints to console.

**resume command:**

Accepts conversation ID argument. Calls manager.resumeConversation(id). Manager handles loading. CLI receives conversation, stores as active conversation (same as `new` command). User can now `chat` with resumed conversation.

---

## Compact Algorithm Deep Dive

**The compression strategy from Rust:**

Codex's compact algorithm is sophisticated. It doesn't just truncate—it intelligently preserves context while reducing tokens.

### What Gets Kept

**1. Initial Context (Always preserved):**
- System prompt
- Base instructions
- Project context (AGENTS.md content)
- Any "pinned" messages

**2. Selected Recent User Messages (Token budget: 20k):**
- Work backwards from most recent
- Keep complete messages until budget exhausted
- Truncate middle of last message if needed (keep head + tail)
- Preserves user's actual words for continuity

**3. Summary (LLM-generated):**
- Concise summary of older conversation
- Includes: decisions made, information shared, current state, unresolved issues
- Inserted as user message (model sees it as context)

**4. GhostSnapshots (Special items):**
- Tool outputs or state snapshots marked as "ghost"
- Always preserved across compression
- Provide grounding for ongoing work

### What Gets Removed

**Removed entirely:**
- Older assistant messages (replaced by summary)
- Older user messages beyond 20k token budget
- Tool call details (unless in GhostSnapshot)
- Intermediate reasoning (unless recent)

**Result:** History goes from e.g. 150k tokens → 40k tokens while preserving critical context.

### Implementation Details

**Threshold calculation:**
```typescript
const contextWindow = turnContext.client.getModelContextWindow(); // e.g., 128k for GPT-4
const threshold = Math.floor(contextWindow * 0.8); // 80% = 102k tokens
const currentTokens = countHistoryTokens(session.getHistory());

if (currentTokens > threshold) {
  await runCompactTask(session, turnContext);
}
```

**Summarization prompt:**

Located in `codex-rs/core/templates/compact/prompt.md` (port this template):
```markdown
Please provide a concise summary of the conversation so far.

Focus on:
- Key decisions made
- Important information shared  
- Current state of work
- Any unresolved issues or next steps

Keep the summary focused and under 500 tokens. This summary will be used to continue the conversation with compressed history.
```

**User message selection:**
```typescript
const userMessages = collectUserMessages(history); // Extract all user messages
const selected = selectRecentMessages(userMessages, 20_000); // Keep up to 20k tokens
```

**History reconstruction:**
```typescript
const initialContext = session.getInitialContext(); // System prompt, etc.
const newHistory = [
  ...initialContext,
  ...selected.map(msg => ({type: 'message', role: 'user', content: [{type: 'text', text: msg}]})),
  {type: 'message', role: 'user', content: [{type: 'text', text: `Summary: ${summary}`}]}
];

// Add GhostSnapshots
const ghosts = history.filter(item => item.type === 'ghost_snapshot');
newHistory.push(...ghosts);

session.replaceHistory(newHistory);
```

**Persistence:**
```typescript
await recorder.appendCompacted(conversationId, {
  timestamp: Date.now(),
  summary: summaryText,
  truncatedCount: truncatedCount
});
```

Rollout file records that compression happened. Resume knows history was compacted.

### Error Handling in Compact

**Context still exceeded after compression:**
- Remove oldest items from compacted history
- Retry compression
- If history down to 1 item and still exceeds → fail with error
- User must start new conversation

**LLM call fails:**
- Retry with backoff (network errors)
- If max retries exceeded → fail, keep original history
- User sees warning: "Compression failed, conversation may hit limits soon"

**Corrupted summary:**
- If LLM returns empty or malformed summary → use fallback: "(summary unavailable)"
- Don't fail compression—better to have placeholder than crash

---

## CLI Command Implementation

### list Command

```typescript
// src/cli/commands/list.ts
import {program} from 'commander';
import {getConversationManager} from '../manager';

program
  .command('list')
  .description('List saved conversations')
  .action(async () => {
    const manager = getConversationManager();
    const conversations = await manager.listConversations();
    
    if (conversations.length === 0) {
      console.log('\nNo saved conversations.');
      console.log('Create one: cody new\n');
      return;
    }
    
    console.log('\nSaved Conversations:\n');
    
    for (const conv of conversations) {
      const date = new Date(conv.updatedAt).toLocaleString();
      const provider = conv.provider || 'unknown';
      const model = conv.model || 'unknown';
      
      console.log(`  ${conv.id}`);
      console.log(`    Provider: ${provider} (${model})`);
      console.log(`    Updated: ${date}`);
      console.log();
    }
    
    console.log(`Total: ${conversations.length} conversation(s)\n`);
  });
```

### resume Command

```typescript
// src/cli/commands/resume.ts
program
  .command('resume <conversationId>')
  .description('Resume saved conversation')
  .action(async (conversationId: string) => {
    const manager = getConversationManager();
    
    try {
      const conversation = await manager.resumeConversation(conversationId);
      
      // Set as active conversation
      setActiveConversation(conversationId, conversation);
      
      console.log(`✓ Resumed conversation: ${conversationId}`);
      console.log(`  Continue chatting: cody chat "your message"\n`);
      
    } catch (err) {
      if (err.code === 'CONVERSATION_NOT_FOUND') {
        console.error(`✗ Conversation '${conversationId}' not found`);
        console.error(`  List available: cody list\n`);
        process.exit(1);
      }
      
      if (err.code === 'CORRUPTED_ROLLOUT') {
        console.error(`✗ Conversation '${conversationId}' has corrupted data`);
        console.error(`  ${err.message}\n`);
        process.exit(1);
      }
      
      throw err; // Unknown error
    }
  });
```

---

## Mock Implementation Guide

### Mock RolloutRecorder

For testing without filesystem:

```typescript
// tests/mocks/rollout-recorder.ts

export function createMockRecorder(): MockRolloutRecorder {
  const storage = new Map<string, string>(); // conversationId → JSONL content
  
  return {
    async appendTurn(id: string, turn: RolloutTurn): Promise<void> {
      const line = JSON.stringify({
        timestamp: turn.timestamp,
        items: turn.items,
        metadata: turn.metadata
      });
      
      const existing = storage.get(id) || '';
      storage.set(id, existing + line + '\n');
    },
    
    async readRollout(id: string): Promise<RolloutTurn[]> {
      const content = storage.get(id);
      if (!content) {
        throw new Error(`Conversation ${id} not found`);
      }
      
      const lines = content.trim().split('\n');
      return lines.map(line => JSON.parse(line));
    },
    
    async list(): Promise<ConversationMetadata[]> {
      const result = [];
      for (const [id, content] of storage.entries()) {
        const firstLine = content.split('\n')[0];
        const turn = JSON.parse(firstLine);
        result.push({
          id,
          updatedAt: turn.timestamp,
          provider: turn.metadata?.provider,
          model: turn.metadata?.model
        });
      }
      return result;
    },
    
    // Test helpers
    getBuffer(id: string): string {
      return storage.get(id) || '';
    },
    
    setBuffer(id: string, content: string): void {
      storage.set(id, content);
    }
  };
}
```

### Mock for Compact (LLM Summarization)

```typescript
// Extend createMockClient to handle summarization

export function createMockClientWithSummarization(
  conversationResponses: ResponseItems[][],
  summaryResponse: string = 'Summary of conversation'
): ModelClient {
  let callIndex = 0;
  
  return {
    sendMessage: vi.fn().mockImplementation(async (req: ChatRequest) => {
      // Check if this is summarization prompt
      const isSummary = req.messages.some(m => 
        m.content.includes('provide a concise summary')
      );
      
      if (isSummary) {
        // Return summary
        return [{
          type: 'message',
          role: 'assistant',
          content: [{type: 'text', text: summaryResponse}]
        }];
      }
      
      // Regular conversation
      return conversationResponses[callIndex++] || defaultResponse;
    })
  };
}
```

**Usage:**
```typescript
const mockClient = createMockClientWithSummarization(
  [/* conversation responses */],
  'User asked about X, we discussed Y, decided Z'
);
```

---

## Reference Locations

**Ported code:**
- RolloutRecorder: `codex-ts/src/core/rollout/` (Phase 2 port)
- ConversationManager: `codex-ts/src/core/conversation-manager/`
- Session: `codex-ts/src/core/codex/session.ts`

**Rust source (for compact port):**
- Algorithm: `codex-rs/core/src/codex/compact.rs`
- Summarization prompt: `codex-rs/core/templates/compact/prompt.md`
- Tests: `codex-rs/core/tests/suite/compact.rs`

**CLI commands:**
- Existing: `codex-ts/src/cli/commands/` (new, chat, set-provider, etc.)
- Add list and resume here

---

## Implementation Notes

**Key decisions to document in DECISIONS.md:**

1. **Storage path:** Default ~/.codex/conversations/ or ~/.cody/conversations/? Match existing config location.

2. **Compact threshold:** 80% of context window (Rust default) or different? Consider model-specific thresholds.

3. **User message token budget:** 20k tokens (Rust default) or adjust? Affects how much recent context preserved.

4. **Summarization model:** Use same model as conversation or different (cheaper) model for summarization? Recommend: same model (consistency).

5. **Compact visibility:** Should user see "Compacting..." message or silent? Recommend: visible (user knows why there's a pause).

6. **Resume validation:** Should resume verify provider/model match current config or allow mismatch? Recommend: allow mismatch (user might want to resume with different model).

7. **JSONL format:** Match Rust exactly or simplify? Recommend: match Rust (enables cross-compatibility).

8. **Compact persistence:** Record CompactedItem in rollout or just rebuild history silently? Recommend: record (rollout shows compression history).

**Error handling philosophy:** Persistence errors should not crash conversation. If appendTurn() fails, log warning but continue. User loses that turn's persistence but conversation continues. Resume errors should be clear and actionable.

---

## Verification Approach

### Mocked-Service Testing (Automated)

Tests in `tests/mocked-service/phase-5-persistence.test.ts` using vitest.

```typescript
describe('Phase 5: Persistence & Resume', () => {
  let mockRecorder: MockRolloutRecorder;
  let mockClient: MockModelClient;
  let manager: ConversationManager;

  beforeEach(() => {
    mockRecorder = createMockRecorder();
    mockClient = createMockClientWithSummarization(
      [/* conversation responses */],
      'Summary of earlier conversation'
    );
    manager = new ConversationManager({
      client: mockClient,
      recorder: mockRecorder
    });
  });

  describe('Auto-Save', () => {
    it('appends turn to JSONL after each message', async () => {
      const {conversation, conversationId} = await manager.newConversation(config);
      
      await conversation.submit([{type: 'text', text: 'First'}]);
      await conversation.nextEvent();
      
      await conversation.submit([{type: 'text', text: 'Second'}]);
      await conversation.nextEvent();
      
      const saved = mockRecorder.getBuffer(conversationId);
      const lines = saved.trim().split('\n');
      
      expect(lines.length).toBeGreaterThanOrEqual(2); // At least 2 turns
    });

    it('includes metadata in rollout', async () => {
      const {conversation, conversationId} = await manager.newConversation({
        provider: 'openai',
        api: 'responses',
        model: 'gpt-4'
      });
      
      await conversation.submit([{type: 'text', text: 'Test'}]);
      await conversation.nextEvent();
      
      const saved = mockRecorder.getBuffer(conversationId);
      const firstLine = JSON.parse(saved.trim().split('\n')[0]);
      
      expect(firstLine.metadata.provider).toBe('openai');
      expect(firstLine.metadata.model).toBe('gpt-4');
    });
  });

  describe('Resume', () => {
    it('reconstructs conversation from JSONL', async () => {
      // Create and save conversation
      const {conversation: conv1, conversationId} = await manager.newConversation(config);
      await conv1.submit([{type: 'text', text: 'First message'}]);
      await conv1.nextEvent();
      await conv1.submit([{type: 'text', text: 'Second message'}]);
      await conv1.nextEvent();
      
      const rollout = mockRecorder.getBuffer(conversationId);
      
      // Resume in fresh manager
      const manager2 = new ConversationManager({
        client: mockClient,
        recorder: mockRecorder
      });
      
      const conv2 = await manager2.resumeConversation(conversationId);
      
      // Verify history loaded
      const history = conv2.getHistory(); // Or equivalent method
      expect(history.length).toBeGreaterThan(0);
      
      // Verify can continue conversation
      await conv2.submit([{type: 'text', text: 'Third message'}]);
      const event = await conv2.nextEvent();
      expect(event.msg).toBeDefined();
    });

    it('throws on missing conversation', async () => {
      await expect(
        manager.resumeConversation('nonexistent-id')
      ).rejects.toThrow('not found');
    });

    it('handles corrupted JSONL', async () => {
      mockRecorder.setBuffer('test-id', 'invalid json\n{broken');
      
      await expect(
        manager.resumeConversation('test-id')
      ).rejects.toThrow();
    });

    it('handles empty rollout file', async () => {
      mockRecorder.setBuffer('test-id', '');
      
      await expect(
        manager.resumeConversation('test-id')
      ).rejects.toThrow('no turns');
    });
  });

  describe('List', () => {
    it('returns all saved conversations', async () => {
      // Create 3 conversations
      const ids = [];
      for (let i = 0; i < 3; i++) {
        const {conversationId} = await manager.newConversation(config);
        ids.push(conversationId);
      }
      
      const list = await manager.listConversations();
      
      expect(list.length).toBe(3);
      expect(list.map(c => c.id)).toEqual(expect.arrayContaining(ids));
    });

    it('returns empty array when no conversations', async () => {
      const list = await manager.listConversations();
      expect(list).toEqual([]);
    });
  });

  describe('Compact', () => {
    it('does not compact when under threshold', async () => {
      // Mock tokenizer returning low count
      const mockTokenizer = {countTokens: () => 1000};
      const session = createSessionWithTokenizer(mockTokenizer);
      
      await session.processMessage('test');
      
      // Verify summarization not called
      expect(mockClient.sendMessage).toHaveBeenCalledTimes(1); // Just the regular message
    });

    it('triggers compact when over threshold', async () => {
      // Mock tokenizer returning high count (>80% of context)
      const mockTokenizer = {countTokens: () => 100_000}; // Exceeds 80k threshold for 100k context
      const session = createSessionWithTokenizer(mockTokenizer);
      
      // Add many messages to history
      for (let i = 0; i < 20; i++) {
        await session.processMessage(`message ${i}`);
      }
      
      // Verify summarization called
      const calls = mockClient.sendMessage.mock.calls;
      const summaryCalls = calls.filter(call => 
        call[0].messages.some(m => m.content.includes('concise summary'))
      );
      expect(summaryCalls.length).toBeGreaterThan(0);
    });

    it('rebuilds history with summary', async () => {
      // Trigger compact
      const session = await createLargeHistorySession();
      await compact.runCompactTask(session, turnContext);
      
      const newHistory = session.getHistory();
      
      // Verify structure
      expect(newHistory.length).toBeLessThan(session.originalHistoryLength);
      
      // Verify summary present
      const summaryItem = newHistory.find(item =>
        item.type === 'message' &&
        item.role === 'user' &&
        item.content.some(c => c.text.includes('Summary'))
      );
      expect(summaryItem).toBeDefined();
    });

    it('preserves GhostSnapshots across compression', async () => {
      const history = [
        /* regular items */,
        {type: 'ghost_snapshot', data: 'important state'},
        /* more items */
      ];
      
      const compacted = await compact.runCompactTask(session, turnContext);
      const newHistory = session.getHistory();
      
      const ghost = newHistory.find(item => item.type === 'ghost_snapshot');
      expect(ghost).toBeDefined();
      expect(ghost.data).toBe('important state');
    });

    it('retries if compacted history still too large', async () => {
      // Mock: first attempt fails with CONTEXT_WINDOW_EXCEEDED
      // Second attempt succeeds after removing oldest item
      
      // Verify: truncatedCount incremented, retry happened
    });

    it('persists CompactedItem to rollout', async () => {
      await compact.runCompactTask(session, turnContext);
      
      const rollout = mockRecorder.getBuffer(conversationId);
      const lines = rollout.trim().split('\n');
      const compactedLine = lines.find(line => {
        const item = JSON.parse(line);
        return item.type === 'compacted';
      });
      
      expect(compactedLine).toBeDefined();
    });
  });
});
```

---

## Quality Gates

**Before marking Phase 5 complete:**

1. **Mocked-service tests:** All passing (~15 tests covering save/resume/list/compact)
2. **Unit tests:** Baseline maintained (1,876+)
3. **TypeScript:** 0 errors
4. **ESLint:** 0 errors
5. **Format:** No changes
6. **Combined:** All checks pass in sequence

**Manual verification:** Follow manual-test-script.md (test save, resume, list, compact with real CLI).

**Code review:**
- Stage 1 (Traditional): JSONL parsing robustness, file path security, compact algorithm correctness, error handling
- Stage 2 (Port Validation): Rollout format matches Rust, compact algorithm matches Rust, resume preserves state correctly

---

## Summary

Phase 5 adds critical persistence and compression capabilities. Persistence enables users to save work and return later. Compact enables long conversations without hitting context limits. Both are essential Codex functionality that should have been in the initial port. The compact algorithm is sophisticated (intelligent compression, retry logic, GhostSnapshot preservation) and requires careful porting from Rust source. Focus on correctness (match Rust behavior), comprehensive testing (mock filesystem and LLM calls), and clear error handling (persistence failures shouldn't crash conversations).


---

TEST CONDITIONS:

# Phase 5: Test Conditions

**Test Framework:** Vitest (mocked-service)
**Test File:** `tests/mocked-service/phase-5-persistence.test.ts`
**Mocks:** `tests/mocks/rollout-recorder.ts`, `tests/mocks/model-client.ts` (extended for summarization)

---

## Suite 1: Auto-Save (Persistence)

### Test 1: Appends Turn After Each Message
- **Setup:** Mock recorder (in-memory), create conversation
- **Execute:** Send two messages (await responses)
- **Verify:** Recorder.appendTurn called twice, buffer has 2+ JSONL lines

### Test 2: Includes Metadata in Rollout
- **Setup:** Create conversation with provider=openai, model=gpt-4
- **Execute:** Send message
- **Verify:** First JSONL line has metadata.provider='openai', metadata.model='gpt-4'

### Test 3: Handles Append Failure Gracefully
- **Setup:** Mock recorder that throws on appendTurn
- **Execute:** Send message
- **Verify:** Conversation continues (doesn't crash), warning logged

---

## Suite 2: Resume

### Test 4: Reconstructs Conversation from JSONL
- **Setup:** Create conversation, send 2 messages, get rollout buffer
- **Execute:** Create new manager, resumeConversation(id) with saved rollout
- **Verify:** Conversation loaded, history.length > 0, can send new message

### Test 5: Throws on Missing Conversation
- **Setup:** Empty recorder
- **Execute:** resumeConversation('nonexistent-id')
- **Verify:** Throws error with message "Conversation nonexistent-id not found"

### Test 6: Handles Corrupted JSONL
- **Setup:** Recorder with invalid JSON: 'invalid json\n{broken'
- **Execute:** resumeConversation('test-id')
- **Verify:** Throws error mentioning parsing failure

### Test 7: Handles Empty Rollout File
- **Setup:** Recorder with empty string for conversation
- **Execute:** resumeConversation('test-id')
- **Verify:** Throws error "Conversation test-id has no turns"

### Test 8: Preserves Provider/Model from Rollout
- **Setup:** Rollout with metadata.provider='anthropic'
- **Execute:** Resume conversation
- **Verify:** Resumed conversation uses anthropic provider (metadata preserved)

---

## Suite 3: List

### Test 9: Returns All Saved Conversations
- **Setup:** Create 3 conversations with mock recorder
- **Execute:** manager.listConversations()
- **Verify:** Returns array with 3 items, IDs match created conversations

### Test 10: Returns Empty Array When No Conversations
- **Setup:** Empty recorder
- **Execute:** manager.listConversations()
- **Verify:** Returns []

### Test 11: Includes Metadata in List
- **Setup:** Create conversation with provider/model metadata
- **Execute:** manager.listConversations()
- **Verify:** Returned items have provider, model, updatedAt fields

---

## Suite 4: Compact Algorithm

### Test 12: Does Not Compact When Under Threshold
- **Setup:** Mock tokenizer returning low count (e.g., 1000 tokens), context window 100k
- **Execute:** Send message
- **Verify:** Summarization not called (mockClient.sendMessage called once for regular message only)

### Test 13: Triggers Compact When Over Threshold
- **Setup:** Mock tokenizer returning high count (e.g., 85k tokens), context window 100k (exceeds 80k threshold)
- **Execute:** Send message
- **Verify:** Summarization called (mockClient.sendMessage called with prompt containing "concise summary")

### Test 14: Rebuilds History with Summary
- **Setup:** Session with large history, trigger compact
- **Execute:** runCompactTask()
- **Verify:** 
  - New history shorter than original
  - Summary item present (role=user, content includes "Summary")
  - Initial context preserved (system prompt still there)

### Test 15: Preserves Recent User Messages
- **Setup:** History with 10 user messages, trigger compact
- **Execute:** runCompactTask()
- **Verify:** 
  - Most recent user messages present in new history
  - Older user messages replaced by summary
  - User message count reduced but recent ones kept

### Test 16: Preserves GhostSnapshots
- **Setup:** History with GhostSnapshot items, trigger compact
- **Execute:** runCompactTask()
- **Verify:** GhostSnapshot items still present in new history (not removed by compression)

### Test 17: Truncates Middle of Long Messages
- **Setup:** User message with 50k tokens (exceeds 20k budget), trigger compact
- **Execute:** selectRecentMessages() with that message
- **Verify:** Message truncated (head + tail preserved, middle replaced with "... truncated ...")

### Test 18: Retries When Compacted History Still Too Large
- **Setup:** Mock: first compact attempt throws CONTEXT_WINDOW_EXCEEDED, second succeeds
- **Execute:** runCompactTask()
- **Verify:** 
  - Oldest item removed from history
  - Retry happened (2 summarization calls)
  - Eventually succeeds

### Test 19: Persists CompactedItem to Rollout
- **Setup:** Trigger compact
- **Execute:** runCompactTask()
- **Verify:** Rollout contains CompactedItem (type='compacted', summary text present)

### Test 20: Handles Summarization Failure
- **Setup:** Mock LLM returns empty or throws error
- **Execute:** runCompactTask()
- **Verify:** 
  - Uses fallback summary: "(summary unavailable)"
  - Doesn't crash
  - History still compacted (with placeholder summary)

---

## CLI Command Tests

### Test 21: list Command Output
- **Setup:** Mock recorder with 3 conversations
- **Execute:** Run `cody list` (capture stdout)
- **Verify:** Output shows all 3 IDs, provider, model, timestamps

### Test 22: list Command When Empty
- **Setup:** Empty recorder
- **Execute:** Run `cody list`
- **Verify:** Output: "No saved conversations", suggests "cody new"

### Test 23: resume Command Success
- **Setup:** Saved conversation in recorder
- **Execute:** Run `cody resume <id>`
- **Verify:** Prints "✓ Resumed conversation: <id>", sets active conversation

### Test 24: resume Command Missing ID
- **Execute:** Run `cody resume nonexistent`
- **Verify:** Error message, suggests "cody list", exit code 1

---

## Mock Strategy

**Mock RolloutRecorder:**
- In-memory Map: conversationId → JSONL content string
- appendTurn: Serializes turn, appends to string
- readRollout: Parses string, returns RolloutTurn[]
- list: Extracts metadata from first line of each conversation
- Test helpers: getBuffer(id), setBuffer(id, content) for setup/verification

**Mock ModelClient (extended for summarization):**
- Detect summarization prompt (contains "concise summary")
- Return preset summary for summarization calls
- Return regular responses for conversation calls
- Track call count to verify summarization triggered

**Mock Tokenizer:**
- countTokens() returns configurable value
- For threshold tests: return high count to trigger compact
- For non-compact tests: return low count

**Mock Filesystem:**
- Not needed (RolloutRecorder mock handles all I/O)
- Tests run entirely in-memory

---

## Verification Checklist

**Automated tests pass:**
- [ ] Auto-save tests: 3 tests
- [ ] Resume tests: 5 tests
- [ ] List tests: 3 tests
- [ ] Compact tests: 9 tests
- [ ] CLI command tests: 4 tests
- [ ] Total: 24 tests, all passing in <3 seconds

**Quality gates:**
- [ ] TypeScript: 0 errors
- [ ] ESLint: 0 errors
- [ ] Format: no changes
- [ ] Combined: All checks pass

**Manual verification:**
- [ ] Follow manual-test-script.md
- [ ] Test save/resume flow
- [ ] Test list command
- [ ] Test compact with long conversation (if possible)

**Code review:**
- [ ] Stage 1: JSONL robustness, file security, compact correctness, error handling
- [ ] Stage 2: Rollout format matches Rust, compact algorithm matches Rust, state preservation correct

---

**All checks ✓ → Phase 5 test conditions complete**


---

TASKS (update source/checklist.md as you work):

# Phase 5: Persistence & Resume + Compact – Task Checklist

**Status:** Not Started
**Estimated Code:** ~920 lines (CLI 110, resume logic 110, compact 450, tests 150, mocks 100)

---

## Setup & Planning

- [ ] Review Phase 4 implementation (auth methods working)
- [ ] Read TECH-APPROACH Section 6 (Phase 5 technical approach)
- [ ] Read phase-5/source/design.md (implementation details)
- [ ] Inspect ported RolloutRecorder: `codex-ts/src/core/rollout/`
- [ ] Read Rust compact source: `codex-rs/core/src/codex/compact.rs`
- [ ] Read Rust compact tests: `codex-rs/core/tests/suite/compact.rs`
- [ ] Locate summarization prompt template: `codex-rs/core/templates/compact/prompt.md`

---

## RolloutRecorder Integration

### Session Auto-Save

- [ ] Locate Session.processMessage() in ported code
- [ ] Add recorder.appendTurn() call after turn completes
- [ ] Pass conversation ID, timestamp, ResponseItems, metadata
- [ ] Handle append errors gracefully (log warning, don't crash)
- [ ] Verify writes happen after tool execution (full turn captured)

### ConversationManager Constructor

- [ ] Inject RolloutRecorder into ConversationManager
- [ ] Pass recorder to Session during conversation creation
- [ ] Default baseDir: ~/.cody/conversations/ (or ~/.codex/conversations/)
- [ ] Document storage path decision in DECISIONS.md

---

## CLI Commands

### list Command

- [ ] Create `src/cli/commands/list.ts`
- [ ] Implement:
  - [ ] Call manager.listConversations()
  - [ ] Format output (ID, provider, model, timestamp)
  - [ ] Handle empty list (print helpful message)
  - [ ] Sort by updatedAt (most recent first)
- [ ] Output format:
  - [ ] Clear table or list
  - [ ] Timestamps human-readable
  - [ ] Total count at bottom

### resume Command

- [ ] Create `src/cli/commands/resume.ts`
- [ ] Implement:
  - [ ] Accept conversationId argument
  - [ ] Call manager.resumeConversation(id)
  - [ ] Set as active conversation
  - [ ] Print confirmation with provider/model info
- [ ] Error handling:
  - [ ] Missing ID → helpful error, suggest `cody list`
  - [ ] Corrupted rollout → error with details
  - [ ] Empty rollout → error "no turns"

---

## Resume Logic

### ConversationManager.resumeConversation()

- [ ] Implement method:
  - [ ] Call recorder.readRollout(id)
  - [ ] Parse RolloutTurn[] → ResponseItem[]
  - [ ] Extract metadata (provider, model, last timestamp)
  - [ ] Create Session with pre-loaded history
  - [ ] Create CodexConversation wrapper
  - [ ] Return conversation
- [ ] Error handling:
  - [ ] File not found → ConversationNotFoundError
  - [ ] Parse error → CorruptedRolloutError with line number
  - [ ] Empty file → EmptyRolloutError

### ConversationManager.listConversations()

- [ ] Implement method:
  - [ ] Call recorder.list()
  - [ ] Return conversation metadata array
  - [ ] Sort by updatedAt descending

---

## Compact Module (Port from Rust)

### Create Module Structure

- [ ] Create `src/core/codex/compact.ts`
- [ ] Port types:
  - [ ] CompactedItem
  - [ ] CompactConfig (thresholds, budgets)
- [ ] Port constants:
  - [ ] COMPACT_USER_MESSAGE_MAX_TOKENS = 20_000
  - [ ] COMPACT_THRESHOLD = 0.8 (80% of context)

### Core Functions

- [ ] **runCompactTask()** - Main entry point
  - [ ] Check threshold
  - [ ] Get summary from LLM
  - [ ] Collect user messages
  - [ ] Build compacted history
  - [ ] Replace session history
  - [ ] Persist CompactedItem
  - [ ] Emit events (compacting, completed, warning)

- [ ] **needsCompaction()** - Threshold check
  - [ ] Count history tokens
  - [ ] Compare to 80% of context window
  - [ ] Return boolean

- [ ] **getSummary()** - LLM summarization
  - [ ] Build summarization prompt
  - [ ] Send to ModelClient
  - [ ] Extract summary text from response
  - [ ] Handle empty or error responses

- [ ] **collectUserMessages()** - Extract user messages
  - [ ] Filter history for role=user
  - [ ] Extract text content
  - [ ] Return string array

- [ ] **buildCompactedHistory()** - Reconstruct history
  - [ ] Start with initial context
  - [ ] Add selected user messages
  - [ ] Add summary message
  - [ ] Add GhostSnapshots
  - [ ] Return new history array

- [ ] **selectRecentMessages()** - Token budget allocation
  - [ ] Work backwards from most recent
  - [ ] Keep messages until budget exhausted
  - [ ] Truncate last message if needed
  - [ ] Return selected messages

- [ ] **truncateMiddle()** - Message truncation
  - [ ] Keep head (first 40%)
  - [ ] Keep tail (last 40%)
  - [ ] Replace middle with "... truncated ..."
  - [ ] Return truncated string

### Retry Logic

- [ ] Implement retry with backoff:
  - [ ] Catch CONTEXT_WINDOW_EXCEEDED
  - [ ] Remove oldest item from history
  - [ ] Increment truncatedCount
  - [ ] Retry compression
  - [ ] Max iterations: history.length (eventually must succeed or fail)
- [ ] Implement network retry:
  - [ ] Catch network errors
  - [ ] Backoff: 250ms, 500ms, 1s, 2s, 4s
  - [ ] Max retries from provider config
  - [ ] Give up after max

### Integration with Session

- [ ] Add compact check to Session.processMessage():
  - [ ] After turn complete, before return
  - [ ] Call compact.needsCompaction()
  - [ ] If true, call compact.runCompactTask()
  - [ ] Await completion
- [ ] Add Session.replaceHistory() method:
  - [ ] Replace internal history array
  - [ ] Maintain other session state
- [ ] Add Session.getInitialContext() method:
  - [ ] Return system prompt + base instructions
  - [ ] Used by compact to preserve initial context

---

## Mocked-Service Tests (TDD)

- [ ] Create `tests/mocked-service/phase-5-persistence.test.ts`
- [ ] Create/extend mocks:
  - [ ] tests/mocks/rollout-recorder.ts (in-memory JSONL)
  - [ ] tests/mocks/model-client.ts (add summarization detection)
  - [ ] tests/mocks/tokenizer.ts (configurable token counts)

### Auto-Save Suite

- [ ] Test 1: Appends turn after each message
- [ ] Test 2: Includes metadata in rollout
- [ ] Test 3: Handles append failure gracefully

### Resume Suite

- [ ] Test 4: Reconstructs conversation from JSONL
- [ ] Test 5: Throws on missing conversation
- [ ] Test 6: Handles corrupted JSONL
- [ ] Test 7: Handles empty rollout
- [ ] Test 8: Preserves provider/model metadata

### List Suite

- [ ] Test 9: Returns all saved conversations
- [ ] Test 10: Returns empty array when none
- [ ] Test 11: Includes metadata in list

### Compact Suite

- [ ] Test 12: Does not compact under threshold
- [ ] Test 13: Triggers compact over threshold
- [ ] Test 14: Rebuilds history with summary
- [ ] Test 15: Preserves recent user messages
- [ ] Test 16: Preserves GhostSnapshots
- [ ] Test 17: Truncates middle of long messages
- [ ] Test 18: Retries when still too large
- [ ] Test 19: Persists CompactedItem to rollout
- [ ] Test 20: Handles summarization failure

### CLI Command Suite

- [ ] Test 21: list command output
- [ ] Test 22: list when empty
- [ ] Test 23: resume command success
- [ ] Test 24: resume missing ID

- [ ] All 24 tests passing
- [ ] Tests run fast (<3 seconds)
- [ ] No skipped tests

---

## Quality Gates

### Code Quality

- [ ] Run: `npm run format` → no changes
- [ ] Run: `npm run lint` → 0 errors
- [ ] Run: `npx tsc --noEmit` → 0 errors

### Testing

- [ ] Run: `npm test` → all passing (1,876+ baseline + 24 new)
- [ ] Verify: 0 skipped tests
- [ ] Verify: Compact tests actually test compression logic

### Combined Verification

- [ ] Run: `npm run format && npm run lint && npx tsc --noEmit && npm test`
- [ ] All commands succeed
- [ ] Save output for verification

---

## Manual Verification

- [ ] Follow `manual-test-script.md`
- [ ] Test save/resume flow (5 scenarios)
- [ ] Test list command
- [ ] Test compact with long conversation (if feasible)
- [ ] Verify JSONL format matches Rust (compare sample files)

---

## Code Review

### Stage 1: Traditional Review

- [ ] JSONL parsing robustness (handle malformed JSON)
- [ ] File path security (no path traversal)
- [ ] Compact algorithm correctness (matches Rust logic)
- [ ] Error handling comprehensive (all failure modes covered)
- [ ] Token counting accurate (affects compact threshold)
- [ ] Summarization prompt quality (produces good summaries)

### Stage 2: Port Validation Review

- [ ] Rollout format matches Rust exactly (field names, structure)
- [ ] Compact algorithm matches Rust (same steps, same logic)
- [ ] Resume preserves state correctly (history, metadata, context)
- [ ] GhostSnapshot handling correct (preserved across compression)
- [ ] Retry logic matches Rust (same backoff, same max retries)

---

## Documentation

- [ ] Update DECISIONS.md:
  - [ ] Storage path chosen
  - [ ] Compact threshold (80% or different)
  - [ ] User message token budget (20k or adjusted)
  - [ ] Summarization model (same as conversation or different)
  - [ ] Compact visibility (user sees message or silent)
  - [ ] Resume validation (allow provider mismatch or enforce)
  - [ ] JSONL format compatibility with Rust
  - [ ] Compact persistence strategy
  - [ ] Claude keyring path found (from Phase 4 if discovered)
- [ ] Update checklist (mark completed)
- [ ] Verify phase ready for Phase 6

---

## Final Verification

- [ ] All tasks completed
- [ ] All tests passing (mocked-service)
- [ ] All quality gates passed
- [ ] Manual verification complete
- [ ] Code review complete (both stages)
- [ ] Documentation updated
- [ ] JSONL files compatible with Rust Codex
- [ ] Ready to commit and proceed

---

**Phase 5 complete when all items checked ✓**


---

STANDARDS:

See docs/core/dev-standards.md for complete coding standards.
See docs/core/contract-testing-tdd-philosophy.md for testing approach.

Key requirements:
- TypeScript strict mode, no any types
- ESLint 0 errors
- Prettier formatted
- Mocked-service tests at library boundaries
- Mock all external dependencies


---

EXECUTION WORKFLOW:

EXECUTION WORKFLOW:

1. Read all reference documents (project context, phase design, standards, test conditions)
2. Review checklist (understand all tasks)
3. Write mocked-service tests FIRST (TDD):
   - Create test file based on test-conditions.md
   - Implement mocks (ModelClient, Config, etc.)
   - Write tests for each functional condition
   - Run tests (should fail - nothing implemented)
4. Implement code to pass tests:
   - Create files listed in checklist
   - Follow design.md implementation specifics
   - Run tests after each component
   - Iterate until all tests green
5. Manual functional testing:
   - Follow manual-test-script.md
   - Execute each test case
   - Verify expected behavior
   - Document any issues
6. Quality verification:
   - Run: npm run format (fix formatting)
   - Run: npm run lint (fix errors)
   - Run: npx tsc --noEmit (fix type errors)
   - Run: npm test (verify all pass)
   - Run combined: npm run format && npm run lint && npx tsc --noEmit && npm test
   - All must pass before proceeding
7. Update artifacts:
   - Update checklist.md (check off completed tasks)
   - Update decisions.md (log implementation choices with rationale)
8. Final verification:
   - All checklist items checked
   - All quality gates pass
   - Manual tests successful
   - Decisions documented
9. Commit and push
10. Report completion, ready for quality verifier and code reviewer

DO NOT declare phase complete until all steps verified.


---

MANUAL VERIFICATION:

# Phase 5: Manual Test Script

**Goal:** Verify conversations persist automatically, can be resumed with full history, and compact algorithm prevents context overflow

**Prerequisites:**
- Phase 4 complete (auth methods working)
- API keys configured
- CLI built and available

---

## Setup

**1. Clean slate:**
```bash
# Backup existing conversations (if any)
mv ~/.cody/conversations ~/.cody/conversations.backup

# Create fresh directory
mkdir -p ~/.cody/conversations
```

**2. Set provider:**
```bash
cody set-provider openai --api responses --model gpt-4o-mini
cody set-auth openai-api-key
```

---

## Test 1: Auto-Save During Conversation

**Objective:** Verify conversations automatically save to JSONL

**Steps:**
1. Create conversation:
   ```bash
   cody new
   ```
   **Expected:** Prints conversation ID (e.g., "conv_abc123")
   **Record ID for later tests**

2. Send first message:
   ```bash
   cody chat "Hello, my name is Alice"
   ```
   **Expected:** Response received

3. Check JSONL file created:
   ```bash
   ls ~/.cody/conversations/
   ```
   **Expected:** File exists: `conv_abc123.jsonl`

4. Inspect JSONL content:
   ```bash
   cat ~/.cody/conversations/conv_abc123.jsonl
   ```
   **Expected:**
   - At least 1 line (JSON object)
   - Contains "Alice" in user message
   - Valid JSON (can parse)

5. Send second message:
   ```bash
   cody chat "What's my name?"
   ```
   **Expected:** Model responds with "Alice"

6. Check JSONL updated:
   ```bash
   cat ~/.cody/conversations/conv_abc123.jsonl | wc -l
   ```
   **Expected:** At least 2 lines (2+ turns saved)

**Success criteria:**
- [ ] JSONL file created automatically
- [ ] File updates after each turn
- [ ] Content is valid JSON
- [ ] User messages preserved in file

---

## Test 2: List Saved Conversations

**Objective:** Verify list command shows saved conversations

**Steps:**
1. Create additional conversations:
   ```bash
   cody new
   cody chat "test conversation 2"
   
   cody new
   cody chat "test conversation 3"
   ```

2. List all conversations:
   ```bash
   cody list
   ```
   **Expected output:**
   ```
   Saved Conversations:
   
     conv_abc123
       Provider: openai (gpt-4o-mini)
       Updated: [timestamp]
   
     conv_def456
       Provider: openai (gpt-4o-mini)
       Updated: [timestamp]
   
     conv_ghi789
       Provider: openai (gpt-4o-mini)
       Updated: [timestamp]
   
   Total: 3 conversation(s)
   ```

3. Verify all 3 conversations listed
4. Verify timestamps are recent
5. Verify provider/model shown

**Success criteria:**
- [ ] All conversations listed
- [ ] Metadata displayed (provider, model, timestamp)
- [ ] Output is readable
- [ ] Total count correct

---

## Test 3: Resume Conversation

**Objective:** Verify can resume saved conversation with full history

**Steps:**
1. Exit CLI (if in REPL mode)

2. Resume first conversation:
   ```bash
   cody resume conv_abc123
   ```
   **Expected:** Prints "✓ Resumed conversation: conv_abc123"

3. Test history loaded:
   ```bash
   cody chat "What did I tell you my name was?"
   ```
   **Expected:** Model responds "Alice" (remembers from Test 1)

4. Continue conversation:
   ```bash
   cody chat "What's 2+2?"
   ```
   **Expected:** Model responds "4"

5. Exit and resume again:
   ```bash
   # Exit CLI
   cody resume conv_abc123
   cody chat "What math question did I just ask?"
   ```
   **Expected:** Model remembers "2+2" question

**Success criteria:**
- [ ] Resume loads conversation
- [ ] History preserved (model remembers earlier messages)
- [ ] Can continue conversation seamlessly
- [ ] Multiple resume cycles work

---

## Test 4: Resume Missing Conversation

**Objective:** Verify helpful error when conversation doesn't exist

**Steps:**
1. Attempt to resume nonexistent ID:
   ```bash
   cody resume conv_nonexistent
   ```
   **Expected:**
   - Error: "✗ Conversation 'conv_nonexistent' not found"
   - Suggests: "List available: cody list"
   - Exit code 1

**Success criteria:**
- [ ] Error message clear
- [ ] Suggests helpful action
- [ ] Doesn't crash

---

## Test 5: Compact with Long Conversation

**Objective:** Verify compact algorithm triggers and works correctly

**Note:** This test is challenging without generating a truly long conversation. We'll simulate by sending many messages.

**Steps:**
1. Create new conversation:
   ```bash
   cody new
   ```

2. Send many messages (15-20 messages):
   ```bash
   for i in {1..20}; do
     cody chat "This is message number $i. Please respond with a detailed explanation of what number this is and count from 1 to $i."
   done
   ```
   **Expected:** 
   - All messages succeed
   - Responses get progressively longer
   - Eventually may see "Compacting conversation history..." message

3. Check if compact triggered:
   ```bash
   cat ~/.cody/conversations/conv_*.jsonl | grep -i compact
   ```
   **Expected:** If history large enough, CompactedItem present in rollout

4. Test history after compact:
   ```bash
   cody chat "What was the first message I sent?"
   ```
   **Expected:**
   - Model may not remember exact first message (compressed)
   - But should have general context from summary
   - Recent messages (15-20) should be remembered

**Success criteria:**
- [ ] Long conversation doesn't crash
- [ ] Compact triggers when threshold exceeded (if reached)
- [ ] History compresses successfully
- [ ] Recent context preserved
- [ ] Conversation continues after compact

**Note:** If conversation doesn't reach compact threshold with 20 messages, that's OK. Compact will be tested in mocked-service tests with mocked tokenizer. This manual test validates the conversation doesn't break with many turns.

---

## Test 6: Resume After Compact

**Objective:** Verify resume works with compacted conversations

**Steps:**
1. Using conversation from Test 5 (if compacted):
   ```bash
   # Exit CLI
   cody resume conv_[id from Test 5]
   ```
   **Expected:** Resumes successfully

2. Test history:
   ```bash
   cody chat "Summarize our conversation so far"
   ```
   **Expected:**
   - Model provides summary
   - Summary includes recent messages
   - May include summary from compact (if triggered)

**Success criteria:**
- [ ] Resume works after compact
- [ ] History coherent
- [ ] Can continue conversation

---

## Test 7: JSONL Format Compatibility

**Objective:** Verify JSONL format matches Rust Codex

**Steps:**
1. Create conversation in Cody:
   ```bash
   cody new
   cody chat "test message"
   ```

2. Copy JSONL file:
   ```bash
   cp ~/.cody/conversations/conv_*.jsonl /tmp/cody-test.jsonl
   ```

3. If you have Rust Codex available, try to resume in Rust:
   ```bash
   # In Rust Codex
   codex resume conv_[id]
   ```
   **Expected:** Rust Codex can read our JSONL (format compatible)

4. If Rust Codex not available, manually inspect format:
   ```bash
   cat /tmp/cody-test.jsonl | jq .
   ```
   **Expected:**
   - Valid JSON per line
   - Fields match Rust format (timestamp, items, metadata)
   - ResponseItem structure matches protocol

**Success criteria:**
- [ ] JSONL format valid
- [ ] Compatible with Rust Codex (if testable)
- [ ] Fields match expected schema

---

## Test 8: Error Recovery

**Objective:** Verify graceful handling of persistence errors

**Steps:**
1. Make conversations directory read-only:
   ```bash
   chmod 444 ~/.cody/conversations
   ```

2. Attempt to create conversation:
   ```bash
   cody new
   cody chat "test"
   ```
   **Expected:**
   - Warning: "Failed to save turn: permission denied"
   - Conversation continues (doesn't crash)
   - Response still displayed

3. Restore permissions:
   ```bash
   chmod 755 ~/.cody/conversations
   ```

**Success criteria:**
- [ ] Persistence errors don't crash conversation
- [ ] User sees warning
- [ ] Conversation continues (in-memory only)

---

## Success Checklist

**Functional verification:**
- [ ] Conversations auto-save to JSONL
- [ ] JSONL files created in correct location
- [ ] list command shows all conversations
- [ ] resume command loads history correctly
- [ ] Multi-turn context preserved across resume
- [ ] Missing conversation error is helpful
- [ ] Compact triggers when threshold exceeded (tested in mocked tests if not manual)
- [ ] Long conversations don't crash
- [ ] JSONL format compatible with Rust

**Quality verification:**
- [ ] All mocked-service tests passing (24 tests)
- [ ] TypeScript: 0 errors
- [ ] ESLint: 0 errors
- [ ] Format: clean
- [ ] Combined: All checks pass

**Code review:**
- [ ] Stage 1 complete (robustness, security, correctness)
- [ ] Stage 2 complete (Rust compatibility, algorithm correctness)

---

**All checks ✓ → Phase 5 functional verification complete**

**Next:** Commit changes, update project logs, proceed to Phase 6

---

## Cleanup

**After all tests pass:**
```bash
# Remove test conversations
rm -rf ~/.cody/conversations/*

# Or restore backup if you had existing conversations
rm -rf ~/.cody/conversations
mv ~/.cody/conversations.backup ~/.cody/conversations
```


---

FINAL QUALITY CHECK:

Before declaring phase complete:

Run: npm run format && npm run lint && npx tsc --noEmit && npm test

ALL must pass. Document results.
Update checklist.md and decisions.md.
Commit and push.
Ready for verification stages.

===== END CODER PROMPT =====
